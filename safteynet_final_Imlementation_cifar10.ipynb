{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "safteynet_final_Imlementation_cifar10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbMiirQk7Zhc"
      },
      "source": [
        "import tensorflow\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras.layers as layers\n",
        "from keras.models import Model\n",
        "from keras.preprocessing import image\n",
        "\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAJSD3fY7zBB"
      },
      "source": [
        "from keras.initializers import glorot_uniform\n",
        "\n",
        "\n",
        "def identity_block(input_tensor, kernel_size, filters, stage, block):\n",
        "    \"\"\" \n",
        "    An identity block.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_tensor:\n",
        "\n",
        "    kernel_size: int\n",
        "        The kernel size of middle conv layer at main path.\n",
        "\n",
        "    filters: list[int]\n",
        "        The filters of 3 conv layer at main path.\n",
        "\n",
        "    stage: int\n",
        "        Current stage label, used for generating layer names.\n",
        "\n",
        "    block: : str\n",
        "        'a','b'..., current block label, used for generating layer names.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Output tensor for the block.\n",
        "    \"\"\"\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "    \n",
        "    # for batch normalization layer, we assume\n",
        "    # the input data is in channel last format\n",
        "    bn_axis = 3\n",
        "\n",
        "    filters1, filters2, filters3 = filters\n",
        "  \n",
        "    # main path, note that setting the kernel_initializer seed here is only used\n",
        "    # for reproducibility, we techniqually don't need it\n",
        "    x = layers.Conv2D(filters1, kernel_size=(1, 1), strides=(1, 1),\n",
        "                      kernel_initializer=glorot_uniform(seed=0),\n",
        "                      padding='valid', name=conv_name_base + '2a')(input_tensor)\n",
        "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    x = layers.Conv2D(filters2, kernel_size, strides=(1, 1),\n",
        "                      kernel_initializer=glorot_uniform(seed=0),\n",
        "                      padding='same', name=conv_name_base + '2b')(x)\n",
        "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    x = layers.Conv2D(filters3, kernel_size=(1, 1), strides=(1, 1),\n",
        "                      kernel_initializer=glorot_uniform(seed=0),\n",
        "                      padding='valid', name=conv_name_base + '2c')(x)\n",
        "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
        "\n",
        "    # this line is the core component of resnet, the skip connection, i.e.\n",
        "    # having a shortcut to the main path before the activation, when addition\n",
        "    # is performed on convolutional layers, the element-wise addition is performed\n",
        "    # on their feature maps, i.e. channel by channel\n",
        "    x = layers.add([x, input_tensor])\n",
        "    x = layers.Activation('relu')(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsmpzMv37zDh"
      },
      "source": [
        "def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2)):\n",
        "    \"\"\" \n",
        "    A block that has a conv layer at shortcut.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_tensor:\n",
        "    \n",
        "    kernel_size: int\n",
        "        The kernel size of middle conv layer at main path.\n",
        "\n",
        "    filters: list[int]\n",
        "        The filters of 3 conv layer at main path.\n",
        "\n",
        "    stage: int\n",
        "        Current stage label, used for generating layer names.\n",
        "\n",
        "    block: : str\n",
        "        'a','b'..., current block label, used for generating layer names.\n",
        "        \n",
        "    strides : tuple, default (2, 2)\n",
        "        Strides for the first conv layer in the block.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Output tensor for the block.\n",
        "    \"\"\"\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "    \n",
        "    # for batch normalization layer, we assume\n",
        "    # the input data is in channel last format,\n",
        "    # which is the case if we are using the default\n",
        "    # keras' backend tensorflow\n",
        "    bn_axis = 3\n",
        "\n",
        "    filters1, filters2, filters3 = filters\n",
        "  \n",
        "    # main path, note that setting the kernel_initializer set here is only used\n",
        "    # for reproducibility, we techniqually don't need it\n",
        "    x = layers.Conv2D(filters1, kernel_size=(1, 1), strides=strides,\n",
        "                      kernel_initializer=glorot_uniform(seed=0),\n",
        "                      padding='valid', name=conv_name_base + '2a')(input_tensor)\n",
        "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    x = layers.Conv2D(filters2, kernel_size, strides=(1, 1),\n",
        "                      kernel_initializer=glorot_uniform(seed=0),\n",
        "                      padding='same', name=conv_name_base + '2b')(x)\n",
        "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    x = layers.Conv2D(filters3, kernel_size=(1, 1), strides=(1, 1),\n",
        "                      kernel_initializer=glorot_uniform(seed=0),\n",
        "                      padding='valid', name=conv_name_base + '2c')(x)\n",
        "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
        "    \n",
        "    # we resize the input so its dimension will match the output dimension\n",
        "    # of the main path\n",
        "    shortcut = layers.Conv2D(filters3, kernel_size=(1, 1), strides=strides,\n",
        "                             kernel_initializer=glorot_uniform(seed=0),\n",
        "                             padding='valid', name=conv_name_base + '1')(input_tensor)\n",
        "    shortcut = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '1')(shortcut) \n",
        "\n",
        "    # this line is the core component of resnet, the skip connection, i.e.\n",
        "    # having a shortcut to the main path before the activation\n",
        "    x = layers.add([x, shortcut])\n",
        "    x = layers.Activation('relu')(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOOv-wgt7zG4",
        "outputId": "9f420dd0-8a23-4ee6-ee7d-1d0c22646e10"
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "from keras.utils import np_utils\n",
        "\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print(x_train.shape, 'train samples')\n",
        "print(x_test.shape, 'test samples')\n",
        "print(y_train.shape, 'train labels')\n",
        "print(y_test.shape, 'test labels')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n",
            "170508288/170498071 [==============================] - 6s 0us/step\n",
            "(50000, 32, 32, 3) train samples\n",
            "(10000, 32, 32, 3) test samples\n",
            "(50000, 1) train labels\n",
            "(10000, 1) test labels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcJ9W_E0tqfQ",
        "outputId": "a83df7e8-b9c1-4eb0-f63c-fea03c04e535"
      },
      "source": [
        "y_train\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6],\n",
              "       [9],\n",
              "       [9],\n",
              "       ...,\n",
              "       [9],\n",
              "       [1],\n",
              "       [1]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 899
        },
        "id": "ckr_6gSpvayj",
        "outputId": "4f07a37a-d657-4af7-dca2-4da634f79984"
      },
      "source": [
        "cv2_imshow(x_train[1])\n",
        "print(x_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=32x32 at 0x7FAC43EDCB50>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[ 59  62  63]\n",
            "  [ 43  46  45]\n",
            "  [ 50  48  43]\n",
            "  ...\n",
            "  [158 132 108]\n",
            "  [152 125 102]\n",
            "  [148 124 103]]\n",
            "\n",
            " [[ 16  20  20]\n",
            "  [  0   0   0]\n",
            "  [ 18   8   0]\n",
            "  ...\n",
            "  [123  88  55]\n",
            "  [119  83  50]\n",
            "  [122  87  57]]\n",
            "\n",
            " [[ 25  24  21]\n",
            "  [ 16   7   0]\n",
            "  [ 49  27   8]\n",
            "  ...\n",
            "  [118  84  50]\n",
            "  [120  84  50]\n",
            "  [109  73  42]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[208 170  96]\n",
            "  [201 153  34]\n",
            "  [198 161  26]\n",
            "  ...\n",
            "  [160 133  70]\n",
            "  [ 56  31   7]\n",
            "  [ 53  34  20]]\n",
            "\n",
            " [[180 139  96]\n",
            "  [173 123  42]\n",
            "  [186 144  30]\n",
            "  ...\n",
            "  [184 148  94]\n",
            "  [ 97  62  34]\n",
            "  [ 83  53  34]]\n",
            "\n",
            " [[177 144 116]\n",
            "  [168 129  94]\n",
            "  [179 142  87]\n",
            "  ...\n",
            "  [216 184 140]\n",
            "  [151 118  84]\n",
            "  [123  92  72]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpTyWFno7zKX",
        "outputId": "ac08bfbe-19b3-43c3-da52-4c06eb4863a7"
      },
      "source": [
        "n_classes = 10\n",
        "img_rows, img_cols, img_channel = 32, 32, 3\n",
        "\n",
        "# mnist is grey-scaled image, thus the last dimension, channel size will be 1\n",
        "X_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, img_channel)\n",
        "X_test  =  x_test.reshape(x_test.shape[0], img_rows, img_cols, img_channel)\n",
        "input_shape = img_rows, img_cols, img_channel\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test  = X_test.astype('float32')\n",
        "\n",
        "# images takes values between 0 - 255, we can normalize it\n",
        "# by dividing every number by 255\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "print('train shape:', X_train.shape)\n",
        "\n",
        "# one-hot encode the class (target) vectors\n",
        "Y_train = np_utils.to_categorical(y_train, n_classes)\n",
        "Y_test = np_utils.to_categorical(y_test , n_classes)\n",
        "print('Y_train shape:', Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train shape: (50000, 32, 32, 3)\n",
            "Y_train shape: (10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bewni7QZv9Lf",
        "outputId": "8d53bc3a-8c91-4369-b9e3-a5f085caf9a5"
      },
      "source": [
        "Y_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPsJVHPG77fp"
      },
      "source": [
        "def ResNet(input_shape, n_classes):\n",
        "    \"\"\"\n",
        "    Definition of ResNet\n",
        "    \n",
        "    References\n",
        "    ----------\n",
        "    https://github.com/keras-team/keras-applications/blob/master/keras_applications/resnet50.py\n",
        "    \"\"\"\n",
        "    img_input = layers.Input(shape=input_shape)\n",
        "    \n",
        "    bn_axis = 3\n",
        "    \n",
        "    x = layers.ZeroPadding2D(padding=(3, 3), name='conv1_pad')(img_input)\n",
        "    x = layers.Conv2D(64, (7, 7),\n",
        "                      strides=(2, 2),\n",
        "                      padding='valid',\n",
        "                      kernel_initializer='he_normal',\n",
        "                      name='conv1')(x)\n",
        "    x = layers.BatchNormalization(axis=bn_axis, name='bn_conv1')(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.ZeroPadding2D(padding=(1, 1), name='pool1_pad')(x)\n",
        "    x = layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
        "\n",
        "    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))\n",
        "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\n",
        "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')\n",
        "\n",
        "    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')\n",
        "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b')\n",
        "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c')\n",
        "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='d')\n",
        "    \n",
        "    # the commented out blocks are what's needed to build out the\n",
        "    # full ResNet50 (a ResNet with 50 layers), we won't be needing\n",
        "    # the complexity here\n",
        "    # x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')\n",
        "    # x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b')\n",
        "    # x = identity_block(x, 3, [256, 256, 1024], stage=4, block='c')\n",
        "    # x = identity_block(x, 3, [256, 256, 1024], stage=4, block='d')\n",
        "    # x = identity_block(x, 3, [256, 256, 1024], stage=4, block='e')\n",
        "    # x = identity_block(x, 3, [256, 256, 1024], stage=4, block='f')\n",
        "\n",
        "    # x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a')\n",
        "    # x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\n",
        "    # x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')\n",
        "    \n",
        "    x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n",
        "    img_output = layers.Dense(n_classes, activation='softmax', name='fc' + str(n_classes))(x)\n",
        "    #img_output = layers.Dense(n_classes, kernel_regularizer=tensorflow.keras.regularizers.l2(0.01),activation\n",
        "     #        ='softmax' , name = 'fc' + str(n_classes))(x); \n",
        "    model = Model(inputs=img_input, outputs=img_output, name='resnet')\n",
        "    return model\n",
        "\n",
        "\n",
        "model = ResNet(input_shape, n_classes)\n",
        "#model.compile(optimizer = 'adam', loss = 'squared_hinge', metrics = ['accuracy'])\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "#model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8flYTSS-CYs",
        "outputId": "6db491b9-6c65-48e1-c01e-8c808e480ad5"
      },
      "source": [
        "history = model.fit(X_train, Y_train, epochs=200, batch_size=32, validation_data=(X_test, Y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "1563/1563 [==============================] - 81s 30ms/step - loss: 1.3761 - accuracy: 0.5093 - val_loss: 1.5169 - val_accuracy: 0.4810\n",
            "Epoch 2/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.9676 - accuracy: 0.6585 - val_loss: 1.8940 - val_accuracy: 0.4730\n",
            "Epoch 3/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.7935 - accuracy: 0.7232 - val_loss: 1.4431 - val_accuracy: 0.5543\n",
            "Epoch 4/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.6714 - accuracy: 0.7665 - val_loss: 1.1067 - val_accuracy: 0.6300\n",
            "Epoch 5/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.5793 - accuracy: 0.7976 - val_loss: 0.7931 - val_accuracy: 0.7345\n",
            "Epoch 6/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.4909 - accuracy: 0.8288 - val_loss: 1.0225 - val_accuracy: 0.7044\n",
            "Epoch 7/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.4163 - accuracy: 0.8549 - val_loss: 0.9105 - val_accuracy: 0.7219\n",
            "Epoch 8/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.3511 - accuracy: 0.8761 - val_loss: 0.8876 - val_accuracy: 0.7247\n",
            "Epoch 9/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.2890 - accuracy: 0.8995 - val_loss: 0.9143 - val_accuracy: 0.7351\n",
            "Epoch 10/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.2409 - accuracy: 0.9150 - val_loss: 0.9793 - val_accuracy: 0.7301\n",
            "Epoch 11/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.2061 - accuracy: 0.9271 - val_loss: 1.0231 - val_accuracy: 0.7360\n",
            "Epoch 12/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.1733 - accuracy: 0.9392 - val_loss: 1.2307 - val_accuracy: 0.7212\n",
            "Epoch 13/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.1554 - accuracy: 0.9449 - val_loss: 0.9778 - val_accuracy: 0.7523\n",
            "Epoch 14/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.1348 - accuracy: 0.9526 - val_loss: 0.9947 - val_accuracy: 0.7492\n",
            "Epoch 15/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.1279 - accuracy: 0.9548 - val_loss: 1.0612 - val_accuracy: 0.7491\n",
            "Epoch 16/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.1118 - accuracy: 0.9608 - val_loss: 1.3299 - val_accuracy: 0.7092\n",
            "Epoch 17/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.1087 - accuracy: 0.9627 - val_loss: 1.0696 - val_accuracy: 0.7463\n",
            "Epoch 18/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.0960 - accuracy: 0.9667 - val_loss: 1.0154 - val_accuracy: 0.7729\n",
            "Epoch 19/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.0915 - accuracy: 0.9683 - val_loss: 1.0362 - val_accuracy: 0.7633\n",
            "Epoch 20/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.0875 - accuracy: 0.9691 - val_loss: 1.0645 - val_accuracy: 0.7603\n",
            "Epoch 21/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.0797 - accuracy: 0.9721 - val_loss: 1.0679 - val_accuracy: 0.7692\n",
            "Epoch 22/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.0765 - accuracy: 0.9731 - val_loss: 1.2387 - val_accuracy: 0.7375\n",
            "Epoch 23/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.0770 - accuracy: 0.9730 - val_loss: 1.2368 - val_accuracy: 0.7423\n",
            "Epoch 24/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.0678 - accuracy: 0.9771 - val_loss: 1.1038 - val_accuracy: 0.7653\n",
            "Epoch 25/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.0652 - accuracy: 0.9769 - val_loss: 1.5228 - val_accuracy: 0.7231\n",
            "Epoch 26/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.0627 - accuracy: 0.9782 - val_loss: 1.1941 - val_accuracy: 0.7649\n",
            "Epoch 27/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.0580 - accuracy: 0.9795 - val_loss: 1.1643 - val_accuracy: 0.7741\n",
            "Epoch 28/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.0597 - accuracy: 0.9796 - val_loss: 1.2298 - val_accuracy: 0.7547\n",
            "Epoch 29/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.0572 - accuracy: 0.9798 - val_loss: 1.2910 - val_accuracy: 0.7375\n",
            "Epoch 30/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.0549 - accuracy: 0.9810 - val_loss: 1.1305 - val_accuracy: 0.7712\n",
            "Epoch 31/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.0511 - accuracy: 0.9822 - val_loss: 1.1952 - val_accuracy: 0.7639\n",
            "Epoch 32/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.0487 - accuracy: 0.9830 - val_loss: 1.2124 - val_accuracy: 0.7624\n",
            "Epoch 33/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.0498 - accuracy: 0.9829 - val_loss: 1.1647 - val_accuracy: 0.7656\n",
            "Epoch 34/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.0471 - accuracy: 0.9838 - val_loss: 1.1795 - val_accuracy: 0.7722\n",
            "Epoch 35/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.0448 - accuracy: 0.9849 - val_loss: 1.3599 - val_accuracy: 0.7567\n",
            "Epoch 36/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.0467 - accuracy: 0.9845 - val_loss: 1.3300 - val_accuracy: 0.7506\n",
            "Epoch 37/200\n",
            "1563/1563 [==============================] - 46s 30ms/step - loss: 0.0372 - accuracy: 0.9870 - val_loss: 1.6158 - val_accuracy: 0.7377\n",
            "Epoch 38/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.0433 - accuracy: 0.9852 - val_loss: 1.2166 - val_accuracy: 0.7784\n",
            "Epoch 39/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.0424 - accuracy: 0.9854 - val_loss: 1.4696 - val_accuracy: 0.7464\n",
            "Epoch 40/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.0375 - accuracy: 0.9871 - val_loss: 1.4774 - val_accuracy: 0.7437\n",
            "Epoch 41/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.0402 - accuracy: 0.9853 - val_loss: 1.3811 - val_accuracy: 0.7654\n",
            "Epoch 42/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.0378 - accuracy: 0.9870 - val_loss: 1.2412 - val_accuracy: 0.7795\n",
            "Epoch 43/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.0347 - accuracy: 0.9883 - val_loss: 1.2890 - val_accuracy: 0.7714\n",
            "Epoch 44/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.0367 - accuracy: 0.9877 - val_loss: 1.2838 - val_accuracy: 0.7566\n",
            "Epoch 45/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.0349 - accuracy: 0.9880 - val_loss: 1.3820 - val_accuracy: 0.7549\n",
            "Epoch 46/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.0362 - accuracy: 0.9879 - val_loss: 1.3318 - val_accuracy: 0.7658\n",
            "Epoch 47/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.0340 - accuracy: 0.9886 - val_loss: 1.2693 - val_accuracy: 0.7722\n",
            "Epoch 48/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.0313 - accuracy: 0.9885 - val_loss: 1.3351 - val_accuracy: 0.7632\n",
            "Epoch 49/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.0321 - accuracy: 0.9889 - val_loss: 1.3901 - val_accuracy: 0.7620\n",
            "Epoch 50/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.0330 - accuracy: 0.9883 - val_loss: 1.2390 - val_accuracy: 0.7786\n",
            "Epoch 51/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.0295 - accuracy: 0.9897 - val_loss: 1.4632 - val_accuracy: 0.7530\n",
            "Epoch 52/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.0287 - accuracy: 0.9908 - val_loss: 1.4390 - val_accuracy: 0.7580\n",
            "Epoch 53/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.0301 - accuracy: 0.9890 - val_loss: 1.4833 - val_accuracy: 0.7627\n",
            "Epoch 54/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.0290 - accuracy: 0.9895 - val_loss: 1.3572 - val_accuracy: 0.7747\n",
            "Epoch 55/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.0259 - accuracy: 0.9912 - val_loss: 1.3577 - val_accuracy: 0.7578\n",
            "Epoch 56/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.0302 - accuracy: 0.9897 - val_loss: 1.4194 - val_accuracy: 0.7647\n",
            "Epoch 57/200\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.0233 - accuracy: 0.9921 - val_loss: 1.2565 - val_accuracy: 0.7777\n",
            "Epoch 58/200\n",
            "1563/1563 [==============================] - 46s 30ms/step - loss: 0.0258 - accuracy: 0.9911 - val_loss: 1.6283 - val_accuracy: 0.7485\n",
            "Epoch 59/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0309 - accuracy: 0.9895 - val_loss: 1.3685 - val_accuracy: 0.7712\n",
            "Epoch 60/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0243 - accuracy: 0.9920 - val_loss: 1.4484 - val_accuracy: 0.7653\n",
            "Epoch 61/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0258 - accuracy: 0.9917 - val_loss: 1.3506 - val_accuracy: 0.7692\n",
            "Epoch 62/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0239 - accuracy: 0.9918 - val_loss: 1.3462 - val_accuracy: 0.7751\n",
            "Epoch 63/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0239 - accuracy: 0.9918 - val_loss: 1.3227 - val_accuracy: 0.7816\n",
            "Epoch 64/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0228 - accuracy: 0.9921 - val_loss: 1.4023 - val_accuracy: 0.7633\n",
            "Epoch 65/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0236 - accuracy: 0.9922 - val_loss: 1.3401 - val_accuracy: 0.7690\n",
            "Epoch 66/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0228 - accuracy: 0.9921 - val_loss: 1.4301 - val_accuracy: 0.7588\n",
            "Epoch 67/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0258 - accuracy: 0.9911 - val_loss: 1.4054 - val_accuracy: 0.7763\n",
            "Epoch 68/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0173 - accuracy: 0.9943 - val_loss: 1.4549 - val_accuracy: 0.7701\n",
            "Epoch 69/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0248 - accuracy: 0.9917 - val_loss: 1.4973 - val_accuracy: 0.7775\n",
            "Epoch 70/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0211 - accuracy: 0.9931 - val_loss: 1.3082 - val_accuracy: 0.7823\n",
            "Epoch 71/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0193 - accuracy: 0.9936 - val_loss: 1.4362 - val_accuracy: 0.7748\n",
            "Epoch 72/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0225 - accuracy: 0.9925 - val_loss: 1.3399 - val_accuracy: 0.7797\n",
            "Epoch 73/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0221 - accuracy: 0.9926 - val_loss: 1.4243 - val_accuracy: 0.7774\n",
            "Epoch 74/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0190 - accuracy: 0.9936 - val_loss: 1.3602 - val_accuracy: 0.7788\n",
            "Epoch 75/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0210 - accuracy: 0.9923 - val_loss: 1.4732 - val_accuracy: 0.7733\n",
            "Epoch 76/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0203 - accuracy: 0.9931 - val_loss: 1.4015 - val_accuracy: 0.7750\n",
            "Epoch 77/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0197 - accuracy: 0.9934 - val_loss: 1.4618 - val_accuracy: 0.7767\n",
            "Epoch 78/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0174 - accuracy: 0.9941 - val_loss: 1.5017 - val_accuracy: 0.7721\n",
            "Epoch 79/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0216 - accuracy: 0.9926 - val_loss: 1.4583 - val_accuracy: 0.7806\n",
            "Epoch 80/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0172 - accuracy: 0.9944 - val_loss: 1.4735 - val_accuracy: 0.7777\n",
            "Epoch 81/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0186 - accuracy: 0.9938 - val_loss: 1.5432 - val_accuracy: 0.7673\n",
            "Epoch 82/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0214 - accuracy: 0.9929 - val_loss: 1.2986 - val_accuracy: 0.7846\n",
            "Epoch 83/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0170 - accuracy: 0.9948 - val_loss: 1.4302 - val_accuracy: 0.7751\n",
            "Epoch 84/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0184 - accuracy: 0.9941 - val_loss: 1.4249 - val_accuracy: 0.7743\n",
            "Epoch 85/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0203 - accuracy: 0.9932 - val_loss: 1.3352 - val_accuracy: 0.7832\n",
            "Epoch 86/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0180 - accuracy: 0.9937 - val_loss: 1.3302 - val_accuracy: 0.7788\n",
            "Epoch 87/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0150 - accuracy: 0.9947 - val_loss: 1.5527 - val_accuracy: 0.7499\n",
            "Epoch 88/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0192 - accuracy: 0.9935 - val_loss: 1.4512 - val_accuracy: 0.7648\n",
            "Epoch 89/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0159 - accuracy: 0.9946 - val_loss: 1.3858 - val_accuracy: 0.7905\n",
            "Epoch 90/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0164 - accuracy: 0.9942 - val_loss: 1.4941 - val_accuracy: 0.7748\n",
            "Epoch 91/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0160 - accuracy: 0.9948 - val_loss: 1.5059 - val_accuracy: 0.7655\n",
            "Epoch 92/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0169 - accuracy: 0.9941 - val_loss: 1.5401 - val_accuracy: 0.7624\n",
            "Epoch 93/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0182 - accuracy: 0.9939 - val_loss: 1.5493 - val_accuracy: 0.7663\n",
            "Epoch 94/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0158 - accuracy: 0.9944 - val_loss: 1.4694 - val_accuracy: 0.7822\n",
            "Epoch 95/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0162 - accuracy: 0.9943 - val_loss: 1.5187 - val_accuracy: 0.7674\n",
            "Epoch 96/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0154 - accuracy: 0.9947 - val_loss: 1.3836 - val_accuracy: 0.7837\n",
            "Epoch 97/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0128 - accuracy: 0.9956 - val_loss: 1.5934 - val_accuracy: 0.7733\n",
            "Epoch 98/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0172 - accuracy: 0.9942 - val_loss: 1.3820 - val_accuracy: 0.7827\n",
            "Epoch 99/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0145 - accuracy: 0.9947 - val_loss: 1.3892 - val_accuracy: 0.7901\n",
            "Epoch 100/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0155 - accuracy: 0.9950 - val_loss: 1.5594 - val_accuracy: 0.7821\n",
            "Epoch 101/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0135 - accuracy: 0.9953 - val_loss: 1.6079 - val_accuracy: 0.7786\n",
            "Epoch 102/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0168 - accuracy: 0.9940 - val_loss: 1.5567 - val_accuracy: 0.7841\n",
            "Epoch 103/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0150 - accuracy: 0.9949 - val_loss: 1.6025 - val_accuracy: 0.7614\n",
            "Epoch 104/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0125 - accuracy: 0.9958 - val_loss: 1.5472 - val_accuracy: 0.7755\n",
            "Epoch 105/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0133 - accuracy: 0.9957 - val_loss: 1.5483 - val_accuracy: 0.7787\n",
            "Epoch 106/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0150 - accuracy: 0.9949 - val_loss: 1.5783 - val_accuracy: 0.7693\n",
            "Epoch 107/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0137 - accuracy: 0.9955 - val_loss: 1.5356 - val_accuracy: 0.7729\n",
            "Epoch 108/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0134 - accuracy: 0.9953 - val_loss: 1.5055 - val_accuracy: 0.7811\n",
            "Epoch 109/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0123 - accuracy: 0.9961 - val_loss: 1.4995 - val_accuracy: 0.7744\n",
            "Epoch 110/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0142 - accuracy: 0.9951 - val_loss: 1.5477 - val_accuracy: 0.7793\n",
            "Epoch 111/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0133 - accuracy: 0.9954 - val_loss: 1.5948 - val_accuracy: 0.7753\n",
            "Epoch 112/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0138 - accuracy: 0.9954 - val_loss: 1.5272 - val_accuracy: 0.7790\n",
            "Epoch 113/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0144 - accuracy: 0.9952 - val_loss: 1.5251 - val_accuracy: 0.7706\n",
            "Epoch 114/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0116 - accuracy: 0.9960 - val_loss: 1.6841 - val_accuracy: 0.7674\n",
            "Epoch 115/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0134 - accuracy: 0.9955 - val_loss: 1.7250 - val_accuracy: 0.7657\n",
            "Epoch 116/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0138 - accuracy: 0.9953 - val_loss: 1.6805 - val_accuracy: 0.7705\n",
            "Epoch 117/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0120 - accuracy: 0.9962 - val_loss: 1.4267 - val_accuracy: 0.7890\n",
            "Epoch 118/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0134 - accuracy: 0.9955 - val_loss: 1.5697 - val_accuracy: 0.7706\n",
            "Epoch 119/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0139 - accuracy: 0.9956 - val_loss: 1.5392 - val_accuracy: 0.7821\n",
            "Epoch 120/200\n",
            "1563/1563 [==============================] - 48s 30ms/step - loss: 0.0125 - accuracy: 0.9956 - val_loss: 1.5393 - val_accuracy: 0.7788\n",
            "Epoch 121/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0128 - accuracy: 0.9952 - val_loss: 1.6665 - val_accuracy: 0.7758\n",
            "Epoch 122/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0117 - accuracy: 0.9964 - val_loss: 1.5824 - val_accuracy: 0.7757\n",
            "Epoch 123/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0121 - accuracy: 0.9960 - val_loss: 1.5744 - val_accuracy: 0.7823\n",
            "Epoch 124/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0133 - accuracy: 0.9955 - val_loss: 1.9516 - val_accuracy: 0.7434\n",
            "Epoch 125/200\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.0119 - accuracy: 0.9960 - val_loss: 1.5257 - val_accuracy: 0.7773\n",
            "Epoch 126/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0123 - accuracy: 0.9959 - val_loss: 1.6890 - val_accuracy: 0.7730\n",
            "Epoch 127/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0114 - accuracy: 0.9961 - val_loss: 1.6011 - val_accuracy: 0.7775\n",
            "Epoch 128/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0118 - accuracy: 0.9961 - val_loss: 1.5828 - val_accuracy: 0.7796\n",
            "Epoch 129/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0105 - accuracy: 0.9964 - val_loss: 1.6016 - val_accuracy: 0.7825\n",
            "Epoch 130/200\n",
            "1563/1563 [==============================] - 48s 30ms/step - loss: 0.0134 - accuracy: 0.9952 - val_loss: 1.7298 - val_accuracy: 0.7708\n",
            "Epoch 131/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0098 - accuracy: 0.9964 - val_loss: 1.5921 - val_accuracy: 0.7843\n",
            "Epoch 132/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0123 - accuracy: 0.9960 - val_loss: 1.7371 - val_accuracy: 0.7608\n",
            "Epoch 133/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0108 - accuracy: 0.9964 - val_loss: 1.8362 - val_accuracy: 0.7626\n",
            "Epoch 134/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0108 - accuracy: 0.9962 - val_loss: 1.6905 - val_accuracy: 0.7760\n",
            "Epoch 135/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0106 - accuracy: 0.9962 - val_loss: 1.4645 - val_accuracy: 0.7873\n",
            "Epoch 136/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0110 - accuracy: 0.9965 - val_loss: 1.6009 - val_accuracy: 0.7702\n",
            "Epoch 137/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0120 - accuracy: 0.9960 - val_loss: 1.6412 - val_accuracy: 0.7797\n",
            "Epoch 138/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0123 - accuracy: 0.9958 - val_loss: 1.4910 - val_accuracy: 0.7864\n",
            "Epoch 139/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0119 - accuracy: 0.9966 - val_loss: 1.4721 - val_accuracy: 0.7890\n",
            "Epoch 140/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0091 - accuracy: 0.9970 - val_loss: 1.5272 - val_accuracy: 0.7874\n",
            "Epoch 141/200\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.0116 - accuracy: 0.9962 - val_loss: 1.4983 - val_accuracy: 0.7911\n",
            "Epoch 142/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0104 - accuracy: 0.9969 - val_loss: 1.6017 - val_accuracy: 0.7781\n",
            "Epoch 143/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0117 - accuracy: 0.9960 - val_loss: 1.5606 - val_accuracy: 0.7803\n",
            "Epoch 144/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0099 - accuracy: 0.9966 - val_loss: 1.6853 - val_accuracy: 0.7806\n",
            "Epoch 145/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0098 - accuracy: 0.9967 - val_loss: 1.6567 - val_accuracy: 0.7786\n",
            "Epoch 146/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0103 - accuracy: 0.9965 - val_loss: 1.4999 - val_accuracy: 0.7860\n",
            "Epoch 147/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0114 - accuracy: 0.9959 - val_loss: 1.6671 - val_accuracy: 0.7764\n",
            "Epoch 148/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0085 - accuracy: 0.9973 - val_loss: 1.6660 - val_accuracy: 0.7699\n",
            "Epoch 149/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0105 - accuracy: 0.9966 - val_loss: 1.6023 - val_accuracy: 0.7802\n",
            "Epoch 150/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0093 - accuracy: 0.9966 - val_loss: 1.5914 - val_accuracy: 0.7856\n",
            "Epoch 151/200\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.0107 - accuracy: 0.9964 - val_loss: 1.4985 - val_accuracy: 0.7867\n",
            "Epoch 152/200\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.0106 - accuracy: 0.9962 - val_loss: 1.5192 - val_accuracy: 0.7901\n",
            "Epoch 153/200\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.0090 - accuracy: 0.9968 - val_loss: 1.6699 - val_accuracy: 0.7780\n",
            "Epoch 154/200\n",
            "1563/1563 [==============================] - 48s 30ms/step - loss: 0.0103 - accuracy: 0.9962 - val_loss: 1.7446 - val_accuracy: 0.7715\n",
            "Epoch 155/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0131 - accuracy: 0.9955 - val_loss: 1.5327 - val_accuracy: 0.7832\n",
            "Epoch 156/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0092 - accuracy: 0.9968 - val_loss: 1.6106 - val_accuracy: 0.7782\n",
            "Epoch 157/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0080 - accuracy: 0.9971 - val_loss: 1.6370 - val_accuracy: 0.7821\n",
            "Epoch 158/200\n",
            "1563/1563 [==============================] - 48s 30ms/step - loss: 0.0116 - accuracy: 0.9961 - val_loss: 1.5396 - val_accuracy: 0.7833\n",
            "Epoch 159/200\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.0086 - accuracy: 0.9970 - val_loss: 1.7646 - val_accuracy: 0.7598\n",
            "Epoch 160/200\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.0104 - accuracy: 0.9966 - val_loss: 1.6291 - val_accuracy: 0.7871\n",
            "Epoch 161/200\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.0086 - accuracy: 0.9971 - val_loss: 1.6746 - val_accuracy: 0.7813\n",
            "Epoch 162/200\n",
            "1563/1563 [==============================] - 48s 30ms/step - loss: 0.0095 - accuracy: 0.9969 - val_loss: 1.7839 - val_accuracy: 0.7751\n",
            "Epoch 163/200\n",
            "1563/1563 [==============================] - 48s 30ms/step - loss: 0.0096 - accuracy: 0.9967 - val_loss: 1.7403 - val_accuracy: 0.7723\n",
            "Epoch 164/200\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0111 - accuracy: 0.9964 - val_loss: 1.4844 - val_accuracy: 0.7848\n",
            "Epoch 165/200\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.0071 - accuracy: 0.9976 - val_loss: 1.6680 - val_accuracy: 0.7901\n",
            "Epoch 166/200\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.0115 - accuracy: 0.9963 - val_loss: 1.6375 - val_accuracy: 0.7802\n",
            "Epoch 167/200\n",
            "1563/1563 [==============================] - 48s 30ms/step - loss: 0.0082 - accuracy: 0.9970 - val_loss: 1.7367 - val_accuracy: 0.7710\n",
            "Epoch 168/200\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.0094 - accuracy: 0.9970 - val_loss: 1.6619 - val_accuracy: 0.7845\n",
            "Epoch 169/200\n",
            "1563/1563 [==============================] - 48s 30ms/step - loss: 0.0092 - accuracy: 0.9967 - val_loss: 1.7031 - val_accuracy: 0.7753\n",
            "Epoch 170/200\n",
            "1563/1563 [==============================] - 48s 30ms/step - loss: 0.0071 - accuracy: 0.9976 - val_loss: 1.6676 - val_accuracy: 0.7820\n",
            "Epoch 171/200\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.0077 - accuracy: 0.9974 - val_loss: 1.6274 - val_accuracy: 0.7875\n",
            "Epoch 172/200\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.0095 - accuracy: 0.9970 - val_loss: 1.6551 - val_accuracy: 0.7767\n",
            "Epoch 173/200\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.0084 - accuracy: 0.9973 - val_loss: 1.7152 - val_accuracy: 0.7726\n",
            "Epoch 174/200\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.0088 - accuracy: 0.9974 - val_loss: 1.6393 - val_accuracy: 0.7860\n",
            "Epoch 175/200\n",
            "1563/1563 [==============================] - 48s 30ms/step - loss: 0.0089 - accuracy: 0.9968 - val_loss: 1.6181 - val_accuracy: 0.7916\n",
            "Epoch 176/200\n",
            "1563/1563 [==============================] - 48s 30ms/step - loss: 0.0080 - accuracy: 0.9972 - val_loss: 1.6366 - val_accuracy: 0.7880\n",
            "Epoch 177/200\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.0101 - accuracy: 0.9966 - val_loss: 1.6600 - val_accuracy: 0.7810\n",
            "Epoch 178/200\n",
            "1563/1563 [==============================] - 48s 30ms/step - loss: 0.0085 - accuracy: 0.9971 - val_loss: 1.5813 - val_accuracy: 0.7863\n",
            "Epoch 179/200\n",
            "1563/1563 [==============================] - 48s 30ms/step - loss: 0.0071 - accuracy: 0.9976 - val_loss: 1.6865 - val_accuracy: 0.7854\n",
            "Epoch 180/200\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.0088 - accuracy: 0.9972 - val_loss: 1.6733 - val_accuracy: 0.7868\n",
            "Epoch 181/200\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.0090 - accuracy: 0.9969 - val_loss: 1.5577 - val_accuracy: 0.7958\n",
            "Epoch 182/200\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.0096 - accuracy: 0.9967 - val_loss: 1.6639 - val_accuracy: 0.7879\n",
            "Epoch 183/200\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.0078 - accuracy: 0.9974 - val_loss: 1.6509 - val_accuracy: 0.7782\n",
            "Epoch 184/200\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.0075 - accuracy: 0.9972 - val_loss: 1.9101 - val_accuracy: 0.7714\n",
            "Epoch 185/200\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.0081 - accuracy: 0.9974 - val_loss: 1.6240 - val_accuracy: 0.7898\n",
            "Epoch 186/200\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.0089 - accuracy: 0.9969 - val_loss: 1.7786 - val_accuracy: 0.7705\n",
            "Epoch 187/200\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.0090 - accuracy: 0.9971 - val_loss: 1.6617 - val_accuracy: 0.7886\n",
            "Epoch 188/200\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.0088 - accuracy: 0.9972 - val_loss: 1.7608 - val_accuracy: 0.7714\n",
            "Epoch 189/200\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.0056 - accuracy: 0.9982 - val_loss: 1.7760 - val_accuracy: 0.7791\n",
            "Epoch 190/200\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.0079 - accuracy: 0.9974 - val_loss: 1.6489 - val_accuracy: 0.7918\n",
            "Epoch 191/200\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.0066 - accuracy: 0.9979 - val_loss: 1.7801 - val_accuracy: 0.7820\n",
            "Epoch 192/200\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.0083 - accuracy: 0.9971 - val_loss: 1.7281 - val_accuracy: 0.7874\n",
            "Epoch 193/200\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.0073 - accuracy: 0.9975 - val_loss: 1.8228 - val_accuracy: 0.7717\n",
            "Epoch 194/200\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.0078 - accuracy: 0.9974 - val_loss: 1.6680 - val_accuracy: 0.7852\n",
            "Epoch 195/200\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.0081 - accuracy: 0.9972 - val_loss: 1.6790 - val_accuracy: 0.7865\n",
            "Epoch 196/200\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.0084 - accuracy: 0.9974 - val_loss: 1.7077 - val_accuracy: 0.7797\n",
            "Epoch 197/200\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.0069 - accuracy: 0.9978 - val_loss: 1.7563 - val_accuracy: 0.7816\n",
            "Epoch 198/200\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.0084 - accuracy: 0.9972 - val_loss: 1.6124 - val_accuracy: 0.7914\n",
            "Epoch 199/200\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.0080 - accuracy: 0.9973 - val_loss: 1.6703 - val_accuracy: 0.7894\n",
            "Epoch 200/200\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.0071 - accuracy: 0.9975 - val_loss: 1.7466 - val_accuracy: 0.7848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU5ART7s7zNT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bcd77ca-be6b-43dd-94af-f14fff03016b"
      },
      "source": [
        "loss, accuracy = model.evaluate(X_test, Y_test)\n",
        "print('Loss = ' + str(loss))\n",
        "print('Test Accuracy = ' + str(accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 3s 11ms/step - loss: 1.7466 - accuracy: 0.7848\n",
            "Loss = 1.7465912103652954\n",
            "Test Accuracy = 0.7847999930381775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwGpZ2mQ8Fwd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73fcf8f8-9b01-43db-f438-f6ad16458994"
      },
      "source": [
        "#!pip install -q tensorflow==2.0.0b1\n",
        "# Install bleeding edge version of cleverhans\n",
        "!pip install git+https://github.com/tensorflow/cleverhans.git#egg=cleverhans\n",
        "\n",
        "import cleverhans\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cleverhans\n",
            "  Cloning https://github.com/tensorflow/cleverhans.git to /tmp/pip-install-yrb3wqob/cleverhans_ec91e4fa59cb4567b4ba3933364054fa\n",
            "  Running command git clone -q https://github.com/tensorflow/cleverhans.git /tmp/pip-install-yrb3wqob/cleverhans_ec91e4fa59cb4567b4ba3933364054fa\n",
            "Collecting nose\n",
            "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[K     |████████████████████████████████| 154 kB 13.1 MB/s \n",
            "\u001b[?25hCollecting pycodestyle\n",
            "  Downloading pycodestyle-2.7.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 530 kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from cleverhans) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from cleverhans) (3.2.2)\n",
            "Collecting mnist\n",
            "  Downloading mnist-0.2.2-py2.py3-none-any.whl (3.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from cleverhans) (1.19.5)\n",
            "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.7/dist-packages (from cleverhans) (0.13.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from cleverhans) (1.0.1)\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.7/dist-packages (from cleverhans) (1.9)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from cleverhans) (0.12.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cleverhans) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->cleverhans) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->cleverhans) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->cleverhans) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->cleverhans) (0.10.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability->cleverhans) (4.4.2)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability->cleverhans) (0.4.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability->cleverhans) (1.3.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability->cleverhans) (0.1.6)\n",
            "Building wheels for collected packages: cleverhans\n",
            "  Building wheel for cleverhans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cleverhans: filename=cleverhans-4.0.0-py3-none-any.whl size=92423 sha256=85183486970ceb1c6d49c616e240b40d9fe15363a75b7bee896636b906123588\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-z5g4d7b1/wheels/60/54/1e/97e3fe32d62bd252c9fbbee44a0545028c6018b81c054af3e4\n",
            "Successfully built cleverhans\n",
            "Installing collected packages: pycodestyle, nose, mnist, cleverhans\n",
            "Successfully installed cleverhans-4.0.0 mnist-0.2.2 nose-1.3.7 pycodestyle-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEXj_jju8FzF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "888d54d9-98ce-468e-f784-233efb3e70af"
      },
      "source": [
        "# Import the attack\n",
        "from cleverhans.tf2.attacks import fast_gradient_method\n",
        "\n",
        "#The attack requires the model to ouput the logits\n",
        "logits_model = tf.keras.Model(model.input,model.layers[-1].output)\n",
        "print(logits_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<keras.engine.functional.Functional object at 0x7faaf5989e50>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElPwr6eS-1ME"
      },
      "source": [
        "def adv_generate(X_images):\n",
        "  images = []\n",
        "  labels = []\n",
        "  adv_orig_images = []\n",
        "  adv_orig_labels = []\n",
        "  epsilon = 0.1\n",
        "\n",
        "  for item in X_images:\n",
        "    #cv2_imshow(item)\n",
        "    adv_orig_images.append(item)\n",
        "    adv_orig_labels.append(0)\n",
        "\n",
        "    original_image = tf.convert_to_tensor(item.reshape((1,32,32,3))) \n",
        "    adv_img = fast_gradient_method.fast_gradient_method(logits_model, original_image, epsilon, np.inf, targeted=False)\n",
        "    adv_img = np.reshape(adv_img, (1,32,32,3))\n",
        "\n",
        "    adv_labels = model.predict(adv_img)\n",
        "    images.append(np.reshape(adv_img, (32,32,3)))\n",
        "    #print(adv_labels)\n",
        "    labels.append(np.reshape(adv_labels,(10)))\n",
        "\n",
        "    adv_orig_images.append(np.reshape(adv_img,(32,32,3)))\n",
        "    adv_orig_labels.append(1)\n",
        "\n",
        "  images = np.array(images)\n",
        "  labels = np.array(labels)\n",
        "  adv_orig_images = np.array(adv_orig_images)\n",
        "  adv_orig_labels = np.array(adv_orig_labels)\n",
        "  labels = labels.reshape(-1, 1) \n",
        "  adv_orig_labels = adv_orig_labels.reshape(-1, 1) \n",
        "  #adv_orig_images,adv_orig_labels = shuffle( adv_orig_images, adv_orig_labels, random_state=4)\n",
        "  return images, labels, adv_orig_images, adv_orig_labels\n",
        "  \n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaRJCGJUBlfq"
      },
      "source": [
        "\n",
        "x_train_adv, y_train_adv, X_train_adv_orig, Y_train_adv_orig = adv_generate(x_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COx-PwYHQlUn"
      },
      "source": [
        "x_test_adv, y_test_adv, X_test_adv_orig, Y_test_adv_orig = adv_generate(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBIhbq17TuN3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74785668-fbf3-4f20-8457-3f8d4c072ecf"
      },
      "source": [
        "x_test_adv.shape\n",
        "#X_test_adv_orig.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6TvPB8YVHiR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74be9e9c-81b2-4290-d027-48dd7aab20cf"
      },
      "source": [
        "\n",
        "Y_train_adv_orig.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100000,)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Csz2Wsdev0ki",
        "outputId": "4c3e888a-28bc-494f-f5eb-daa4147e969d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GhkKQ6mzB8x",
        "outputId": "cbd3add1-87a2-48aa-d72c-727ddabbeec1"
      },
      "source": [
        "!pip install pyyaml h5py "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from h5py) (1.19.5)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py) (1.5.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9VHvVxOzH2z",
        "outputId": "bdc00483-3c36-4b62-8f91-a81caf92d3a0"
      },
      "source": [
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "print(tf.version.VERSION)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eo_wg63AzCLX",
        "outputId": "39d1641f-fd3f-4f50-ca43-7a427509eb8c"
      },
      "source": [
        "!mkdir -p saved_model\n",
        "model.save('/content/gdrive/MyDrive/saved_model/SafetyNet_cifar_99_78') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/saved_model/SafetyNet_cifar_99_78/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BO1Erd8z6fI",
        "outputId": "bd4392ff-fec7-4114-a3c9-38d3bd99b4a3"
      },
      "source": [
        "# my_model directory\n",
        "!ls saved_model\n",
        "\n",
        "# Contains an assets folder, saved_model.pb, and variables folder.\n",
        "!ls saved_model/my_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my_model\n",
            "assets\tkeras_metadata.pb  saved_model.pb  variables\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN4xAS7z0FXr"
      },
      "source": [
        "new_model = tf.keras.models.load_model('saved_model/my_model')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ulRBSdX0NVQ",
        "outputId": "ea475616-c390-481b-fb6a-3260580bf9d8"
      },
      "source": [
        "# Evaluate the restored model\n",
        "loss, acc = new_model.evaluate(X_test, Y_test, verbose=2)\n",
        "print('Restored model, accuracy: {:5.2f}%'.format(100 * acc))\n",
        "\n",
        "print(new_model.predict(X_test).shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 3s - loss: 1.7466 - accuracy: 0.7848\n",
            "Restored model, accuracy: 78.48%\n",
            "(10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNaM77oERrXB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8233be3c-a2f3-4d0c-a281-f5c6ca8c1c43"
      },
      "source": [
        "Y_test_adv_orig.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400000, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23_p88bjpheF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "1dddc862-1586-4e12-a622-1b4044715370"
      },
      "source": [
        "cv2_imshow(x_test[5])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=32x32 at 0x7FAAF11C3C50>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOWzjTwBSAA4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "9eb8df2b-49fb-4a9b-e0ef-7d24c281fbb9"
      },
      "source": [
        "cv2_imshow(np.reshape(x_test_adv[4], (32,32,3)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=32x32 at 0x7FAAF0362290>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmeG3Btk8J89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "b36a890e-98e1-4882-9813-1b05dd5663e7"
      },
      "source": [
        "\n",
        "plt.imshow(np.reshape(x_test_adv[4], (32,32,3)).astype(int))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7faaf5378f50>"
            ]
          },
          "metadata": {},
          "execution_count": 116
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2dWYxd15We/3XnoapYE2tgFckiKUrUPISSLVlx2+60oTgNyAoCw34w9GC0GkEbiIHOg+AAsQPkwR3ENvwQOJBjodWB4yFtGxYCIS212rCsdCyLkjWSGkiqSBbJqiJrnu688nAvEUrY/6kSi3WL9vk/gOCtve4+Z999zzrn3P2ftZa5O4QQf/gktnsAQoj2IGcXIibI2YWICXJ2IWKCnF2ImCBnFyImpDbT2cweAPAdAEkA/83dvxH1/kwm5blcNrytJD/vVGv1YHu9Hm4HgEa9QW2pdJLakOJTYsa7MbzBpc3KaonvK8F3ls6lqY12c749j7ClUhHjyEYcPmyyIqRei5jgeo33q9Vq1MZ2VyzmaJ+oMS4uLFNbOhv1vUQc35Uq6cT7JBPhY7i0UkKlXAlO5BU7u5klAfwXAH8CYALAi2b2pLsfZX1yuSwOf+RQ0Jbu7KL7mp6bDbbPzs7TPuUl7kg9Q3xfqd4+arM0mfyoE9US+SIBnH6ZThXSXRlq231wmNryxDkbVX4g1mvcyXp2cqcY3sfnKklOmo06d8xUmn/mxVk+jxcmp6mt2gh/tnvvCR+HAOBlPsann36O2kbGRqgtnw5f5ADg3JnJYHsy30H7dBY7g+2/ffpF2mczt/H3ADju7ifdvQLgRwAe3MT2hBBbyGacfQTAmcv+nmi1CSGuQTb1m30jmNkjAB4BgGyO36YJIbaWzVzZzwLYfdnfo6229+Huj7n7YXc/nElv+blFCEHYjLO/COCgme0zswyAzwN48uoMSwhxtbniS62718zsywD+Dk3p7XF3fzOyU9KQ6giv7uZ39tBuHeVysH12bo726R0Mr1YCwNABvpo9X+KSHUBWrSPkutUSl2rqDb7CvKNrB7XtHOCfLeXhn0qLCxEyZZKPsaO/QG3VCOmzvEbk0mqF9skWo7RNLpdWy3weU5l8sL1vB1dkVpcXuG1xldounJuhtnyG/4RNevizFbu6aZ8Kmd+oINZN3Ve7+1MAntrMNoQQ7UFP0AkRE+TsQsQEObsQMUHOLkRMkLMLERPa+pSLJZNIEckjneXSREdXWGoqzvI+g6O91JbvLFLbQoXLUKkUCSZJ8Gmsr63x7UWcaotEogSAakSUV8LDUlNpZZH2KVW4rVHr5/0WeLDR7GQ4SCmZ4QE5O/eExw4AqQyX5corXM7L5cPfdS4iQq1e4jJfaZXLfJVVrnsN9vHjMdcVDnipRlyLz586F2yvV7kcqiu7EDFBzi5ETJCzCxET5OxCxAQ5uxAxoa2r8clUGjt2DgRtS/Ph1FMAkOsIB2N09vC0Pd3DfPVzORxXAwBIJ/gqbY4EVVQbPHimVuIr1pmIlWmLyLk2N8kVgxw5fZeXl2gfGF/BLSS5KtBZ5PPfqIYHUo3IM8dSWQFAo8ZXwRPJiDx56fAcsxxuAJDP8s88tHsXtY3u3kttwyPh4x4AykRpmBifoH1W18JBYA2PUGqoRQjxB4WcXYiYIGcXIibI2YWICXJ2IWKCnF2ImNBW6S1hQDYVljyMtAPAwFBY7lgsX6R9LCKTbXmBa2+ZBK/ckW6Ez40ekfirUuFBGlHZ7hYu8vx6+SIP5CnlwjJadx/PZ9bRyaWmJeey3GqNy4r1AinzVeHS0NoCz++WyfDrkqX5/BeIbJtN8KCbrgGed+/QHbySDCKOYc/zMSZIRaFCnkuzd913W7D9whkuYevKLkRMkLMLERPk7ELEBDm7EDFBzi5ETJCzCxETNiW9mdk4gCUAdQA1dz8c9f56vY6lhXBpHYuIHDtz+lSwvRhR4H51hudVq1e51JSJiHpbmQ/nVUsUeC68yGitiAiwTESOtL49XEYrdofLRhU6eYQaEvycX69yyagaET5oHv5sy9NcGlq4wMsn3XT3DdTWN8RLh4EMP5vmx0B3F5c2i728bNRanX/X1Qihtacj/H327ObH99JyOPIxSWQ84Oro7J90dy54CyGuCXQbL0RM2KyzO4CnzewlM3vkagxICLE1bPY2/n53P2tmAwCeMbO33P25y9/QOgk8AgD5Dv6IohBia9nUld3dz7b+nwbwcwD3BN7zmLsfdvfD2TxfcBBCbC1X7OxmVjSzzkuvAXwawBtXa2BCiKvLZm7jBwH83JryUQrA/3D3/x3Vod5oYGklHNlUTXBpYvyV14PtI3t58r/OiMiw7iKPavKIZJQLCythQ4S81oiI8uqIGOO+2/dQW/91fdTGpBczfl6fOhWWQwHgzDGe9LC3k0teN99ya7D9yJthGRUA5i/yRJrFzrCkCACJJJcHy+Vw1F6hO1xSDAByWS5TFotcsss772d1Psb+7p3B9tfffJn2efvoO8H2lSUeOXjFzu7uJwHcfqX9hRDtRdKbEDFBzi5ETJCzCxET5OxCxAQ5uxAxoa0JJxuNBlZLa0FbpcElqjKpX1XcxSWofIM/wFOvcH0tYTxpYEcuLLtcmOXJIUtrfF8HbhmjtrE7R6it7DyJJVPYls5xee2df+SPRywvRMhhN/DIvDrCn7trgNc8y0ZcerIJHllYjXhWq3Mk/NTmdJlH33V2cFmumOeybarBx4gal5brpC7eyXfO0D5TJ6aD7dWyar0JEXvk7ELEBDm7EDFBzi5ETJCzCxET2lv+KZFAnpTjWb7IM1sNjYwG28cO7Kd9evI8SOP0ifeo7dxJHqjRuzO8SpsmK88AUBniARyjh4aoLZHm5+FEiSsGVgvnfjv5Eg9oWZklAT4AbriNz/Ghj9xIbedPh1eSuyKW3A/dfT21Jbr4yn++m6sy6UJ4f6VKOJ8gAEzN8hV3A19xTyZ4TsF6gn9nS0thherCNM/J12jwwBqGruxCxAQ5uxAxQc4uREyQswsRE+TsQsQEObsQMaG90lsqiXxvWL7KzPFAjQTCskVHjpfiyXdx+WT/jbyU0OTpSW6bCkshQx08L9kdt3F5avcQz6HnDX4eriV4zrt33zwebL9w+gLtM7gvnAMNAA595GZq6+zjc7y2Vgq2d3XyqJXsYC+1JdIRgTDgwR9Tx8Ofe/f1g7TPWi0shQFAKhEheUUF6zS4LHfxwrlg+9wMl6PzCT73DF3ZhYgJcnYhYoKcXYiYIGcXIibI2YWICXJ2IWLCutKbmT0O4E8BTLv7La22XgA/BjAGYBzA59ydJ2JrkTBDLhWOXkpHSBO1ariET6PO83pZRARSPqKEz4GbuSz30nMvBNvfOnuW9rn1fi5dldNcxkkv8M/W53z8S+gOtt98/UHap/8gl6HSRS6VrazyaLmde8PjyOzgY1/jiiJ68zxq7MQrXC6dOB3O1Xb/oXB5KgBoJMKyIQBEBZt5gpd/qta5tNyohks2Nerh4x4AGsZtjI1c2f8awAMfaHsUwLPufhDAs62/hRDXMOs6e6ve+gdTcT4I4InW6ycAfPYqj0sIcZW50t/sg+5+vvV6Es2KrkKIa5hNL9C5uwOgv2TM7BEzO2JmR8qr/LeQEGJruVJnnzKzYQBo/R9eBQHg7o+5+2F3P5wt8MUZIcTWcqXO/iSAh1uvHwbwi6szHCHEVrER6e2HAD4BoN/MJgB8DcA3APzEzL4E4BSAz21sZwkMJsPROuMRt/j1ejiqqVrmZZDqNS5NJLJcxhm9fozazo+Hk1FOXuQyWXZXuPwQAMzUFqltYIGPv7POk1j25MPyz3Wf/GPap3cXjzZbWOOS0bLxEkrlejhyLHMuQk5a4fO4nA/LUwCQjijZdd2dYSk1189LPM3McBV5tRpRHizDbdkkj8zLkW4J4/Lx8vJSsL3e4PO7rrO7+xeIiR89QohrDj1BJ0RMkLMLERPk7ELEBDm7EDFBzi5ETGhrwslGvY7lubBksLLMI6iYsrIwx6Urj4gYGtgdUWMtzx/8ueXe24Ptt5YO0D7JJA/lWrvIZa3BDI82K9S5JIO55WDz5MlwIkoASCZHqK0rIrFhss7nqlwNy2iZOV4XL5Pi+7p4jsth13VwGa2M8DyWlrjUmyKRmQCwuMLrr5Wdf9dD3fyzNchcpTLcPXcNhpOEjp8IJ68EdGUXIjbI2YWICXJ2IWKCnF2ImCBnFyImyNmFiAltld6QSMAKYSlkaJQnuymXwzJJvcojiSolLvHMTfK6ZwNju6mtpy8cHVac5dNYPsOlkJEMr1VXTfB6YxXjEs+uXeFtVom8AwDVMzQdAS5UeYbFRpJHeXUWw9F3xTyP2EtleK20REQdta4sv2ZdnAnLm5VxLnt6L5cUCxFjTOYjrp1pLueVSRbLsRv20z779oTl0slJLlHqyi5ETJCzCxET5OxCxAQ5uxAxQc4uRExo62p8IplArrsYtGUu8lXOfFd4dTST4sNPJblt7hwvFzQwzINk6slwAEptka/8V+d47rTpOs+hl87xQJiuDr5anCOLvoVOvvJfWuWqRlT676hgI5YjbTnFt5eMCEAByV0IAJm+HmrbvSOsoDQafO6Pvz1BbT2DA9RWTnN1YnmN7y9J3DCf5cdwxcPbc57VXVd2IeKCnF2ImCBnFyImyNmFiAlydiFigpxdiJiwkfJPjwP4UwDT7n5Lq+3rAP4MwKWIkq+6+1PrbavRaGBlJSxF1So8uKNGFI1ag0tG9TqXIFIFXpJpdTEsGQFAbkc4uCPVxXOg3feJP6K2F15+mdr+z5HfUdut1x+ktsGe8FiWZsK56QBgRzcPThkdHKa2tRW+zZn5cGmoUoQEhST/zqZmuFxa6OSy7d7rwuWfrMSPnX0NHjQ0PsuDhlJdu6htpcQ/9/i7J4Lt773zFu0zPPaxYHsi4vK9kSv7XwN4IND+bXe/o/VvXUcXQmwv6zq7uz8HgFfwE0L8XrCZ3+xfNrPXzOxxM+OPMAkhrgmu1Nm/C+AAgDsAnAfwTfZGM3vEzI6Y2ZFSxKOXQoit5Yqc3d2n3L3u7g0A3wNwT8R7H3P3w+5+OFfgz3QLIbaWK3J2M7t8ifYhAG9cneEIIbaKjUhvPwTwCQD9ZjYB4GsAPmFmdwBwAOMA/nwjO2s0GqishXOrFQthWQsAqgjLco0cl0jyXXx7hWK4dA4A1OtckmmQKK+zC7wk0MECl+XuufUuanvp5aPUtlrmY8yTHG+5DI/ISiR4Oalz56aoLZvlUWp7x8aC7d7g+0pHRI3tjigPdj5ijMePhefx+pvvpH0O9N5MbbMv8PyFsxERjlXwzzazGM6Ht6Onn/bZfyBccuzX2Zdon3Wd3d2/EGj+/nr9hBDXFnqCToiYIGcXIibI2YWICXJ2IWKCnF2ImNDWhJMGIEkS4hU6uFTW1Re2lRs80WMmE1ESaOI8tRX7wwkKAWDxXLhfLsMlqN8c5ZFLH7v9bmp76F8+RG0Tp8aprU6iB3OdXAIEV8PQ2cEPkXqDRyqemwhHqWUyPOKwUePbS+X5HA+Ocil1YSYs2V2c5Ekljy8sUtvw0Bi1TUyOU5t38Mi8PTfsCbaPH32P9pmcuBhsr1W4LKsruxAxQc4uREyQswsRE+TsQsQEObsQMUHOLkRMaG+tt0QChXxYeqnVuf7T0xuO/kmUuVRTqvBEGdNnI2p58ZyHqFXDySjzw7z+12ya10P7x1d5Usl/8alPU5uXwpGDAHD6xPFgezbPpc1yhSdD3DXEI6+yEbXI5pfCyShzGV7Dzur8+5yaC0tNAFDP8mtWvhjOobC2wuW1aplHr/3qd+9S2/gqT1ba0c2lwx19YZ8YvWGU9ukfHAy2p9J8P7qyCxET5OxCxAQ5uxAxQc4uREyQswsRE9q7Gp9MIr+jK2ire1SOtPAK47lTPFCgUuSr+40Ut02d5iv1o2PhFdDKGl/57x3hK/VH/+8r1FZ87tfUductvPxTaS28Cp6JyPHXP8SDZCqr4fxoAFCp8ECk/t6+YHvDovLd8RJP9UrEdanCt1kj+6s3uEqSz/KglTPTvPxToo8rF7MX56itNj8fbL/r4+ESTwAw1E9W4yPyAurKLkRMkLMLERPk7ELEBDm7EDFBzi5ETJCzCxETNlL+aTeAvwEwiGa5p8fc/Ttm1gvgxwDG0CwB9Tl35/oCmoEw+Y5C0LZU4lLIe2+HgztWIoIjigWel6zKVT6srPEyQ8l0OKji5Php2mdxlgdHjNx6HbU99ezz1LZU5kEc99x6a7C9XOJBJoWIgpuZND9EFohkBHA5Mh8hASbSPD9dNh9R6ivJx1ghElu5yuejHFECbPf+cNklAFhOcdlrIcEjrHoGybGa5UFDU6VwybFahKS4kSt7DcBfuvtNAD4K4C/M7CYAjwJ41t0PAni29bcQ4hplXWd39/Pu/nLr9RKAYwBGADwI4InW254A8NmtGqQQYvN8qN/sZjYG4E4ALwAYdPdLuZUn0bzNF0Jco2zY2c2sA8BPAXzF3d/3o9HdHQgnhDezR8zsiJkdWV3mSQGEEFvLhpzdzNJoOvoP3P1nreYpMxtu2YcBBB8advfH3P2wux8ukMU5IcTWs66zm5mhWY/9mLt/6zLTkwAebr1+GMAvrv7whBBXi41EvX0MwBcBvG5ml8K0vgrgGwB+YmZfAnAKwOfW25CZIZsKywnnL5yh/U699Xaw/da7b6Z9kimury3VuYzTsWMHtZXWwrna+np5yajTZ05R2/D1e6lt3z+5idqOj/PIvP1j4VJCB/byfZWWudxYq3PJaGBohNrOTYQ/99wilyIz4N9LLaLU1FyEvJkthI83b3B5zWtcvsrkeITdykJYDgOA0X3h7wUA9t4UlvPOznFJd7kUPhajovnWdXZ3fx68Gtgfr9dfCHFtoCfohIgJcnYhYoKcXYiYIGcXIibI2YWICW1NOFmv17EwH47YWl7gEVQdhXA0kUXIJ9ksl4x6e3iU1/mLvLTSCkmwOHaAyyo7dvZQ24l3T1Dbob08uiqR4g8nVTwsyayWuLzWReYXAJZqPJlmpcptha7uYPvFeZ6wcW2OB012dXJJtJDm16yEhaWoniKPsFuqh5N2AkBxhT8F2h0RpbZjkCcevVC+EGxfrnFJER5OihlRvUxXdiHigpxdiJggZxciJsjZhYgJcnYhYoKcXYiY0FbprdGoY3UlLL0VImpU3ffPPhlsP3TjftrnzAyXtSYWeUTc2rtceltbDctXS1UuAe7sCNc8A4CZBk+YeezNt6jt4zffTm39HeFaekszPCKrKyJqz2q8ntvCaljma3YMH1oJHtiGYpHXnCvkuFS2Ro4pAMiSum0N47LhapZvr7DKP8D+YR4FOJPi+5tbCB8H6TyX8mprLLqNi2+6sgsRE+TsQsQEObsQMUHOLkRMkLMLERPauhqfSqfQOxRe+R0+eD3tdwfJ1dbTz4Mjunr56n6GL4Ij1cFzjM1MhVfdGw0esHD61Hlq6y7w8ad3DlHb9Brf3+5iMdierPFV2nqJr7jXSPAPANQRUTaKlGTKGL++rNW4qjE8EDEfPLYGyyvhuZqPmMOS82NgbZ6P8cIazw3o/bysglXC+fWyxYhSWdlwn2Z+WNKHWoQQf1DI2YWICXJ2IWKCnF2ImCBnFyImyNmFiAnrSm9mthvA36BZktkBPObu3zGzrwP4MwCXEmh91d2fitpWo9HA2mo4IGBi+SztV6lOBdv37ttH+4wO9lPbDbtuoLZkgk9JPjMbbC+Xecmd8hIPgFhc4CWNbrueS5G5iJxx89PhgJedKS6TTVzgWuTZiAAaT4dlPgDYPxSWmjoLPKDFkhEBShUedJNKhINdAGB5OSyx1ap87gc7eL64oyvvUtub771Hbfv2RgT5ZMLfZ3WNHztnToVLQ1XKEfNELf+fGoC/dPeXzawTwEtm9kzL9m13/88b2IYQYpvZSK238wDOt14vmdkxADyWTwhxTfKhfrOb2RiAOwG80Gr6spm9ZmaPmxnPmSyE2HY27Oxm1gHgpwC+4u6LAL4L4ACAO9C88n+T9HvEzI6Y2ZG1ZZ4YQgixtWzI2c0sjaaj/8DdfwYA7j7l7nV3bwD4HoB7Qn3d/TF3P+zuh/MdfHFGCLG1rOvs1nyy/vsAjrn7ty5rH77sbQ8BeOPqD08IcbXYyGr8xwB8EcDrZvZKq+2rAL5gZnegKceNA/jz9TZUq9YwMxmWeWo1Ll8dfSssM+yb4nLdfffeTW393TyaaG//KLUlE2Fp6ExESaPdN3IZZ3qClzs6fvxFauvu4RFgXR6ObluK+AV1+jSP1nr71BlqG+jjn62/EJbDdnbznHw93eH8eQBw5nz4GACArgg5r7s3XIZqZYWX0LqwGJZYAWB2hZeGWliMKNcUEY22Ro79yZPHaZ98I/w9W4P70UZW458HEBpppKYuhLi20BN0QsQEObsQMUHOLkRMkLMLERPk7ELEhDaXf3KsroWjcrpyXAp5d/xCsP30e+FoOABYXgyXagKAu++7idp6e/hTv0P9e4LtxTxPHHl6bpzaGqM8amw5x8e/uMLlsFouHN221IiQfnbyiKxUaje1zS1zGarGAtiINAgAi3Pz1NY3yBM2ri0vUNvcQtiWSPFIubMzPArw5eM8sq3/Dl6OLCrR5sQ7Yemzg8iXAJDxcNReQgknhRBydiFigpxdiJggZxciJsjZhYgJcnYhYkJbpbdEIoF8gSQ+rPFEeYl6WE6YmuTJEJ/9xfPU1rWDJzY8eOt11FZIhaOyRjt30j7ZRIPa3m7waLP3BRB/gEyZy1dOEg5WcxEJFvt59NpAjQ9kZXaR2pbIODqcR4atVniCxVSey1DFbJba5ojU997ESdrnrXEebYaICLuBER4x+dqvXqC2Pzp8ONh+9z+9l/b59T88HWxPRSTt1JVdiJggZxciJsjZhYgJcnYhYoKcXYiYIGcXIia0VXqzBJAuhs8vtRrvl+4JR8Tt7eaJFyeOTVLb88+8Sm2FLi6tFIph2bCY5+fMgR08Eipd4MkXT13k8s/iKpfRSvlwwsG5hXDkIAAsVbitNM0jygqrvH5ctdEbbJ/PcSkyk+XRd5UK7ze3zBNEniURcbNpLl/WO/nnGurjx8eF905RWypi/HuuCydATaa4tNzdEY60ZElRAV3ZhYgNcnYhYoKcXYiYIGcXIibI2YWICeuuxptZDsBzALKt9/+tu3/NzPYB+BGAPgAvAfiiu/NoFgBAA95YDVrmZ3jOtfNnw6vFN35kjPaprPDV1vkZHozxy787Qm21RHilu3I9lxJ2Vbmtr4uvxt8wdDO1zS3xFfLp1XD+tCR4WaBCguf/K2fC5ZMA4J3fHaW289PhkljDowdon9mTJ6itUuL1qyxYsKhJfiA8/j033UD79OwJ5xoEgJUSz7uXSPFrZ98wDzbyfPgYmV/iPjG/GJ6POikLBWzsyl4G8Cl3vx3N8swPmNlHAfwVgG+7+3UA5gB8aQPbEkJsE+s6uze5dDpLt/45gE8B+NtW+xMAPrslIxRCXBU2Wp892argOg3gGQAnAMy7+6X7jwkAI1szRCHE1WBDzu7udXe/A8AogHsAHNroDszsETM7YmZHSqvlKxymEGKzfKjVeHefB/BLAPcC6DazSwt8owCCxdLd/TF3P+zuh3MFnlFECLG1rOvsZrbTzLpbr/MA/gTAMTSd/l+13vYwgF9s1SCFEJtnI4EwwwCeMLMkmieHn7j7/zKzowB+ZGb/EcDvAHx/vQ3VqnXMT80FbW+99A7tV1oJ3/4nSakjAOjbzSWjyhr/OXH2XV765zcIB9Ck82naZ3EnD9LomuVj3DXAA2i6O/upLZMOn78LxnO47Szw7e0c47Lc3h08cOVXvwlLmO+t8ACliyvBm0MAQF9E0NPInr3UNjoazqG3excva3VxJnyMAsAyeJ685rp1mM5OXlas3CASW53P/cBIWOVOpfmxuK6zu/trAO4MtJ9E8/e7EOL3AD1BJ0RMkLMLERPk7ELEBDm7EDFBzi5ETDAn5XG2ZGdmFwBcStTVD4DrXO1D43g/Gsf7+X0bx153D9Yja6uzv2/HZkfcPVzkSuPQODSOqz4O3cYLERPk7ELEhO109se2cd+Xo3G8H43j/fzBjGPbfrMLIdqLbuOFiAnb4uxm9oCZvW1mx83s0e0YQ2sc42b2upm9YmY80+TV3+/jZjZtZm9c1tZrZs+Y2but/3mY1NaO4+tmdrY1J6+Y2WfaMI7dZvZLMztqZm+a2b9ptbd1TiLG0dY5MbOcmf3WzF5tjeM/tNr3mdkLLb/5sVlEKGMId2/rPwBJNNNa7QeQAfAqgJvaPY7WWMYB9G/Dfj8O4C4Ab1zW9p8APNp6/SiAv9qmcXwdwL9t83wMA7ir9boTwDsAbmr3nESMo61zAsAAdLRepwG8AOCjAH4C4POt9v8K4F9/mO1ux5X9HgDH3f2kN1NP/wjAg9swjm3D3Z8D8MFA9wfRTNwJtCmBJxlH23H38+7+cuv1EprJUUbQ5jmJGEdb8SZXPcnrdjj7CIAzl/29nckqHcDTZvaSmT2yTWO4xKC7n2+9ngQwuI1j+bKZvda6zd/ynxOXY2ZjaOZPeAHbOCcfGAfQ5jnZiiSvcV+gu9/d7wLwzwH8hZl9fLsHBDTP7IhKe7K1fBfAATRrBJwH8M127djMOgD8FMBX3H3xcls75yQwjrbPiW8iyStjO5z9LIDLcwLRZJVbjbufbf0/DeDn2N7MO1NmNgwArf/DJVW2GHefah1oDQDfQ5vmxMzSaDrYD9z9Z63mts9JaBzbNSetfX/oJK+M7XD2FwEcbK0sZgB8HsCT7R6EmRXNrPPSawCfBvBGdK8t5Uk0E3cC25jA85JztXgIbZgTMzM0cxgec/dvXWZq65ywcbR7TrYsyWu7Vhg/sNr4GTRXOk8A+HfbNIb9aCoBrwJ4s53jAPBDNG8Hq2j+9voSmjXzngXwLoC/B9C7TeP47wBeB/Aams423IZx3I/mLfprAF5p/ftMu+ckYhxtnRMAt6GZxPU1NE8s//6yY/a3AI4D+J8Ash9mu3qCToiYECpqGdEAAAAtSURBVPcFOiFig5xdiJggZxciJsjZhYgJcnYhYoKcXYiYIGcXIibI2YWICf8P0Qs5HAAGJ2IAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVTZnQJu_TCQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30585de4-1151-40fb-ab70-8f39080a45ec"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "Y_train_adv_orig.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14400000, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYlZc5xH02Ue",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "9a23f042-e5c1-49f9-f4e8-380196340b5c"
      },
      "source": [
        "cv2_imshow(np.reshape(x_train_adv[4], (32,32,3)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=32x32 at 0x7FAAEF9E2690>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0cIIiPQDV77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95ab8a83-ffef-4d9a-b4fe-bdd11931b311"
      },
      "source": [
        "x_train_adv.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrxvYhSI_dWs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e9d9990-5ac0-4f5c-c139-80ea48407a67"
      },
      "source": [
        "Y_train_adv_orig"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, ..., 1, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOJ8VP-JTQJz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01263298-1103-4792-a014-29df4caf7625"
      },
      "source": [
        "#X_train_adv_orig.shape[0]\n",
        "X_train_adv_orig.shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100000"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKmwAUplPhcC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aba2304-cf44-488c-b1bd-464d24c7a559"
      },
      "source": [
        "img_rows, img_cols, img_channel = 32, 32, 3\n",
        "\n",
        "# mnist is grey-scaled image, thus the last dimension, channel size will be 1\n",
        "X_train_adv_orig = X_train_adv_orig.reshape(X_train_adv_orig.shape[0], img_rows, img_cols, img_channel)\n",
        "X_test_adv_orig =  X_test_adv_orig.reshape(X_test_adv_orig.shape[0], img_rows, img_cols, img_channel)\n",
        "input_shape = img_rows, img_cols, img_channel\n",
        "\n",
        "X_train_adv_orig = X_train_adv_orig.astype('float32')\n",
        "X_test_adv_orig  = X_test_adv_orig.astype('float32')\n",
        "\n",
        "# images takes values between 0 - 255, we can normalize it\n",
        "# by dividing every number by 255\n",
        "X_train_adv_orig /= 255\n",
        "X_test_adv_orig /= 255\n",
        "print('train shape:', X_train_adv_orig.shape)\n",
        "# one-hot encode the class (target) vectors\n",
        "#Y_train_adv_orig = np_utils.to_categorical(Y_train_adv_orig, nb_classes1)\n",
        "#Y_test_adv_orig = np_utils.to_categorical(Y_train_adv_orig, nb_classes1)\n",
        "print('Y_train shape:', Y_train_adv_orig.shape)\n",
        "print(X_test_adv_orig.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train shape: (100000, 32, 32, 3)\n",
            "Y_train shape: (100000,)\n",
            "(20000, 32, 32, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "so4ysbU6GRmo"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "columnTransformer = ColumnTransformer([('encoder', OneHotEncoder(), [0])],     remainder='passthrough')\n",
        "Y_train_adv_orig=np.array(columnTransformer.fit_transform(Y_train_adv_orig),dtype=np.float)\n",
        "Y_train_adv_orig.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RG83TNf8GMau"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "columnTransformer = ColumnTransformer([('encoder', OneHotEncoder(), [0])],     remainder='passthrough')\n",
        "Y_test_adv_orig=np.array(columnTransformer.fit_transform(Y_test_adv_orig),dtype=np.float)\n",
        "Y_test_adv_orig.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJk2dLFnKJDW"
      },
      "source": [
        "def ResNet1(input_shape, n_classes):\n",
        "    \"\"\"\n",
        "    Definition of ResNet\n",
        "    \n",
        "    References\n",
        "    ----------\n",
        "    https://github.com/keras-team/keras-applications/blob/master/keras_applications/resnet50.py\n",
        "    \"\"\"\n",
        "    img_input1 = layers.Input(shape=input_shape)\n",
        "    \n",
        "    bn_axis = 3\n",
        "    \n",
        "    x = layers.ZeroPadding2D(padding=(3, 3), name='conv1_pad')(img_input1)\n",
        "    x = layers.Conv2D(64, (7, 7),\n",
        "                      strides=(2, 2),\n",
        "                      padding='valid',\n",
        "                      kernel_initializer='he_normal',\n",
        "                      name='conv1')(x)\n",
        "    x = layers.BatchNormalization(axis=bn_axis, name='bn_conv1')(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.ZeroPadding2D(padding=(1, 1), name='pool1_pad')(x)\n",
        "    x = layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
        "\n",
        "    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))\n",
        "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\n",
        "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')\n",
        "\n",
        "    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')\n",
        "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b')\n",
        "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c')\n",
        "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='d')\n",
        "    \n",
        "    # the commented out blocks are what's needed to build out the\n",
        "    # full ResNet50 (a ResNet with 50 layers), we won't be needing\n",
        "    # the complexity here\n",
        "    # x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')\n",
        "    # x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b')\n",
        "    # x = identity_block(x, 3, [256, 256, 1024], stage=4, block='c')\n",
        "    # x = identity_block(x, 3, [256, 256, 1024], stage=4, block='d')\n",
        "    # x = identity_block(x, 3, [256, 256, 1024], stage=4, block='e')\n",
        "    # x = identity_block(x, 3, [256, 256, 1024], stage=4, block='f')\n",
        "\n",
        "    # x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a')\n",
        "    # x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\n",
        "    # x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')\n",
        "    \n",
        "    x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n",
        "    #img_output = layers.Dense(n_classes, activation='softmax', name='fc' + str(n_classes))(x)\n",
        "    img_output1 = layers.Dense(n_classes, activation='softmax', name='fc' + str(n_classes))(x)\n",
        "             #='linear' , name = 'fc' + str(n_classes))(x); \n",
        "    model1 = Model(inputs=img_input1, outputs=img_output1, name='resnet1')\n",
        "    return model1\n",
        "\n",
        "model1 = ResNet1(input_shape, 2)\n",
        "model1.compile(optimizer = 'adam', loss ='categorical_entropy', metrics = ['accuracy'])\n",
        "#model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "#model1.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQkA5pa6yYiQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d76518a-deeb-427f-c981-912a3f7a58c8"
      },
      "source": [
        "Y_train_adv_orig.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14400000, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMzQXVN_yljg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b3fbba8-9e1e-40c1-b64a-c9f76ad1cc5f"
      },
      "source": [
        "len(Y_train_adv_orig)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14400000"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tVZ_c4nGGHT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kVlmL9TQOBg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "0f1acfaa-8b23-4326-b9fa-f417d2e7a27f"
      },
      "source": [
        "history1 = model1.fit(X_train_adv_orig, Y_train_adv_orig, epochs=200, batch_size=32, validation_data=(X_test_adv_orig, Y_test_adv_orig))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-95-994501d9d531>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_adv_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train_adv_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_adv_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test_adv_orig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1146\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1381\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1383\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0m_check_data_cardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;31m# If batch_size is not passed but steps is, calculate from the input data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1647\u001b[0m           label, \", \".join(str(i.shape[0]) for i in tf.nest.flatten(single_data)))\n\u001b[1;32m   1648\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1649\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 100000\n  y sizes: 200000\nMake sure all arrays contain the same number of samples."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCMITrArxsTL"
      },
      "source": [
        "Y_test_adv_orig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WD26ZXQOQOER"
      },
      "source": [
        "\n",
        "loss, accuracy = model1.evaluate(X_test_adv_orig, Y_test_adv_orig)\n",
        "print('Loss = ' + str(loss))\n",
        "print('Test Accuracy = ' + str(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}