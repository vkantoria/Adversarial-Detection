{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "70+_400_lr = 0.001_epochs_safteynet_final_Imlementation_cifar10_categorical_svm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbMiirQk7Zhc"
      },
      "source": [
        "import tensorflow\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras.layers as layers\n",
        "from keras.models import Model\n",
        "from keras.preprocessing import image\n",
        "\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAJSD3fY7zBB"
      },
      "source": [
        "from keras.initializers import glorot_uniform\n",
        "\n",
        "\n",
        "def identity_block(input_tensor, kernel_size, filters, stage, block):\n",
        "    \"\"\" \n",
        "    An identity block.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_tensor:\n",
        "\n",
        "    kernel_size: int\n",
        "        The kernel size of middle conv layer at main path.\n",
        "\n",
        "    filters: list[int]\n",
        "        The filters of 3 conv layer at main path.\n",
        "\n",
        "    stage: int\n",
        "        Current stage label, used for generating layer names.\n",
        "\n",
        "    block: : str\n",
        "        'a','b'..., current block label, used for generating layer names.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Output tensor for the block.\n",
        "    \"\"\"\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "    \n",
        "    # for batch normalization layer, we assume\n",
        "    # the input data is in channel last format\n",
        "    bn_axis = 3\n",
        "\n",
        "    filters1, filters2, filters3 = filters\n",
        "  \n",
        "    # main path, note that setting the kernel_initializer seed here is only used\n",
        "    # for reproducibility, we techniqually don't need it\n",
        "    x = layers.Conv2D(filters1, kernel_size=(1, 1), strides=(1, 1),\n",
        "                      kernel_initializer=glorot_uniform(seed=0),\n",
        "                      padding='valid', name=conv_name_base + '2a')(input_tensor)\n",
        "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    x = layers.Conv2D(filters2, kernel_size, strides=(1, 1),\n",
        "                      kernel_initializer=glorot_uniform(seed=0),\n",
        "                      padding='same', name=conv_name_base + '2b')(x)\n",
        "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    x = layers.Conv2D(filters3, kernel_size=(1, 1), strides=(1, 1),\n",
        "                      kernel_initializer=glorot_uniform(seed=0),\n",
        "                      padding='valid', name=conv_name_base + '2c')(x)\n",
        "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
        "\n",
        "    # this line is the core component of resnet, the skip connection, i.e.\n",
        "    # having a shortcut to the main path before the activation, when addition\n",
        "    # is performed on convolutional layers, the element-wise addition is performed\n",
        "    # on their feature maps, i.e. channel by channel\n",
        "    x = layers.add([x, input_tensor])\n",
        "    x = layers.Activation('relu')(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsmpzMv37zDh"
      },
      "source": [
        "def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2)):\n",
        "    \"\"\" \n",
        "    A block that has a conv layer at shortcut.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_tensor:\n",
        "    \n",
        "    kernel_size: int\n",
        "        The kernel size of middle conv layer at main path.\n",
        "\n",
        "    filters: list[int]\n",
        "        The filters of 3 conv layer at main path.\n",
        "\n",
        "    stage: int\n",
        "        Current stage label, used for generating layer names.\n",
        "\n",
        "    block: : str\n",
        "        'a','b'..., current block label, used for generating layer names.\n",
        "        \n",
        "    strides : tuple, default (2, 2)\n",
        "        Strides for the first conv layer in the block.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Output tensor for the block.\n",
        "    \"\"\"\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "    \n",
        "    # for batch normalization layer, we assume\n",
        "    # the input data is in channel last format,\n",
        "    # which is the case if we are using the default\n",
        "    # keras' backend tensorflow\n",
        "    bn_axis = 3\n",
        "\n",
        "    filters1, filters2, filters3 = filters\n",
        "  \n",
        "    # main path, note that setting the kernel_initializer set here is only used\n",
        "    # for reproducibility, we techniqually don't need it\n",
        "    x = layers.Conv2D(filters1, kernel_size=(1, 1), strides=strides,\n",
        "                      kernel_initializer=glorot_uniform(seed=0),\n",
        "                      padding='valid', name=conv_name_base + '2a')(input_tensor)\n",
        "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    x = layers.Conv2D(filters2, kernel_size, strides=(1, 1),\n",
        "                      kernel_initializer=glorot_uniform(seed=0),\n",
        "                      padding='same', name=conv_name_base + '2b')(x)\n",
        "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    x = layers.Conv2D(filters3, kernel_size=(1, 1), strides=(1, 1),\n",
        "                      kernel_initializer=glorot_uniform(seed=0),\n",
        "                      padding='valid', name=conv_name_base + '2c')(x)\n",
        "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
        "    \n",
        "    # we resize the input so its dimension will match the output dimension\n",
        "    # of the main path\n",
        "    shortcut = layers.Conv2D(filters3, kernel_size=(1, 1), strides=strides,\n",
        "                             kernel_initializer=glorot_uniform(seed=0),\n",
        "                             padding='valid', name=conv_name_base + '1')(input_tensor)\n",
        "    shortcut = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '1')(shortcut) \n",
        "\n",
        "    # this line is the core component of resnet, the skip connection, i.e.\n",
        "    # having a shortcut to the main path before the activation\n",
        "    x = layers.add([x, shortcut])\n",
        "    x = layers.Activation('relu')(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOOv-wgt7zG4",
        "outputId": "8f39cc43-2ecc-481d-8f63-9ed88bfa3004"
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "from keras.utils import np_utils\n",
        "\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print(x_train.shape, 'train samples')\n",
        "print(x_test.shape, 'test samples')\n",
        "print(y_train.shape, 'train labels')\n",
        "print(y_test.shape, 'test labels')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "170508288/170498071 [==============================] - 2s 0us/step\n",
            "(50000, 32, 32, 3) train samples\n",
            "(10000, 32, 32, 3) test samples\n",
            "(50000, 1) train labels\n",
            "(10000, 1) test labels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcJ9W_E0tqfQ",
        "outputId": "6077fbc1-2161-4a6c-b350-919da444e988"
      },
      "source": [
        "print(y_train[2].shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 872
        },
        "id": "ckr_6gSpvayj",
        "outputId": "abdb0446-797f-48d5-8054-0668af6ee876"
      },
      "source": [
        "cv2_imshow(x_train[1])\n",
        "print(x_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=32x32 at 0x7FC5AB8B3F10>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[ 59  62  63]\n",
            "  [ 43  46  45]\n",
            "  [ 50  48  43]\n",
            "  ...\n",
            "  [158 132 108]\n",
            "  [152 125 102]\n",
            "  [148 124 103]]\n",
            "\n",
            " [[ 16  20  20]\n",
            "  [  0   0   0]\n",
            "  [ 18   8   0]\n",
            "  ...\n",
            "  [123  88  55]\n",
            "  [119  83  50]\n",
            "  [122  87  57]]\n",
            "\n",
            " [[ 25  24  21]\n",
            "  [ 16   7   0]\n",
            "  [ 49  27   8]\n",
            "  ...\n",
            "  [118  84  50]\n",
            "  [120  84  50]\n",
            "  [109  73  42]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[208 170  96]\n",
            "  [201 153  34]\n",
            "  [198 161  26]\n",
            "  ...\n",
            "  [160 133  70]\n",
            "  [ 56  31   7]\n",
            "  [ 53  34  20]]\n",
            "\n",
            " [[180 139  96]\n",
            "  [173 123  42]\n",
            "  [186 144  30]\n",
            "  ...\n",
            "  [184 148  94]\n",
            "  [ 97  62  34]\n",
            "  [ 83  53  34]]\n",
            "\n",
            " [[177 144 116]\n",
            "  [168 129  94]\n",
            "  [179 142  87]\n",
            "  ...\n",
            "  [216 184 140]\n",
            "  [151 118  84]\n",
            "  [123  92  72]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpTyWFno7zKX",
        "outputId": "565032d1-72e7-4958-bc8e-c959f6bdedaa"
      },
      "source": [
        "n_classes = 10\n",
        "img_rows, img_cols, img_channel = 32, 32, 3\n",
        "\n",
        "# mnist is grey-scaled image, thus the last dimension, channel size will be 1\n",
        "X_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, img_channel)\n",
        "X_test  = x_test.reshape(x_test.shape[0], img_rows, img_cols, img_channel)\n",
        "input_shape = img_rows, img_cols, img_channel\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test  = X_test.astype('float32')\n",
        "\n",
        "# images takes values between 0 - 255, we can normalize it\n",
        "# by dividing every number by 255\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "print('train shape:', X_train.shape)\n",
        "\n",
        "# one-hot encode the class (target) vectors\n",
        "Y_train = np_utils.to_categorical(y_train, n_classes)\n",
        "Y_test = np_utils.to_categorical(y_test , n_classes)\n",
        "print('Y_test shape:', Y_test.shape)\n",
        "Y_test[0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train shape: (50000, 32, 32, 3)\n",
            "Y_test shape: (10000, 10)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10,)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bewni7QZv9Lf",
        "outputId": "fb81904f-dab5-459d-ea3e-4a8bbd976faa"
      },
      "source": [
        "Y_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPsJVHPG77fp"
      },
      "source": [
        "def ResNet(input_shape, n_classes):\n",
        "    \"\"\"\n",
        "    Definition of ResNet\n",
        "    \n",
        "    References\n",
        "    ----------\n",
        "    https://github.com/keras-team/keras-applications/blob/master/keras_applications/resnet50.py\n",
        "    \"\"\"\n",
        "    img_input = layers.Input(shape=input_shape)\n",
        "    \n",
        "    bn_axis = 3\n",
        "    \n",
        "    x = layers.ZeroPadding2D(padding=(3, 3), name='conv1_pad')(img_input)\n",
        "    x = layers.Conv2D(64, (7, 7),\n",
        "                      strides=(2, 2),\n",
        "                      padding='valid',\n",
        "                      kernel_initializer='he_normal',\n",
        "                      name='conv1')(x)\n",
        "    x = layers.BatchNormalization(axis=bn_axis, name='bn_conv1')(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.ZeroPadding2D(padding=(1, 1), name='pool1_pad')(x)\n",
        "    x = layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
        "\n",
        "    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))\n",
        "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\n",
        "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')\n",
        "\n",
        "    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')\n",
        "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b')\n",
        "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c')\n",
        "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='d')\n",
        "    \n",
        "    # the commented out blocks are what's needed to build out the\n",
        "    # full ResNet50 (a ResNet with 50 layers), we won't be needing\n",
        "    # the complexity here\n",
        "    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')\n",
        "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b')\n",
        "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='c')\n",
        "    #x = identity_block(x, 3, [256, 256, 1024], stage=4, block='d')\n",
        "    # x = identity_block(x, 3, [256, 256, 1024], stage=4, block='e')\n",
        "    # x = identity_block(x, 3, [256, 256, 1024], stage=4, block='f')\n",
        "\n",
        "    # x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a')\n",
        "    # x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\n",
        "    # x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')\n",
        "    \n",
        "    x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n",
        "    #img_output = layers.Dense(n_classes, activation='softmax', name='fc' + str(n_classes))(x)\n",
        "    img_output = layers.Dense(n_classes, kernel_regularizer=tensorflow.keras.regularizers.l2(0.01),activation ='softmax' , name = 'fc' + str(n_classes))(x); \n",
        "    model = Model(inputs=img_input, outputs=img_output, name='resnet')\n",
        "    return model\n",
        "\n",
        "\n",
        "model = ResNet(input_shape, n_classes)\n",
        "model.compile(optimizer = 'adam', loss = 'squared_hinge', metrics = ['accuracy'])\n",
        "#model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "#model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8flYTSS-CYs",
        "outputId": "6ed528e3-7157-4e1a-e35f-2e78539b18ce"
      },
      "source": [
        "history = model.fit(X_train[0:1265], Y_train[0:1265], epochs=50, batch_size=32, validation_data=(X_test[0:565], Y_test[0:565]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "40/40 [==============================] - 6s 156ms/step - loss: 0.9626 - accuracy: 0.8474 - val_loss: 1.0117 - val_accuracy: 0.7165\n",
            "Epoch 2/50\n",
            "40/40 [==============================] - 6s 142ms/step - loss: 0.9510 - accuracy: 0.8751 - val_loss: 1.0204 - val_accuracy: 0.6932\n",
            "Epoch 3/50\n",
            "40/40 [==============================] - 6s 142ms/step - loss: 0.9473 - accuracy: 0.8917 - val_loss: 1.0076 - val_accuracy: 0.7252\n",
            "Epoch 4/50\n",
            "40/40 [==============================] - 6s 142ms/step - loss: 0.9430 - accuracy: 0.8980 - val_loss: 1.0092 - val_accuracy: 0.7203\n",
            "Epoch 5/50\n",
            "40/40 [==============================] - 6s 141ms/step - loss: 0.9368 - accuracy: 0.9083 - val_loss: 1.0180 - val_accuracy: 0.6984\n",
            "Epoch 6/50\n",
            "40/40 [==============================] - 6s 142ms/step - loss: 0.9348 - accuracy: 0.9194 - val_loss: 1.0083 - val_accuracy: 0.7247\n",
            "Epoch 7/50\n",
            "40/40 [==============================] - 6s 142ms/step - loss: 0.9318 - accuracy: 0.9265 - val_loss: 1.0057 - val_accuracy: 0.7298\n",
            "Epoch 8/50\n",
            "40/40 [==============================] - 6s 143ms/step - loss: 0.9301 - accuracy: 0.9273 - val_loss: 1.0105 - val_accuracy: 0.7174\n",
            "Epoch 9/50\n",
            "40/40 [==============================] - 6s 142ms/step - loss: 0.9268 - accuracy: 0.9360 - val_loss: 1.0102 - val_accuracy: 0.7152\n",
            "Epoch 10/50\n",
            "40/40 [==============================] - 6s 142ms/step - loss: 0.9244 - accuracy: 0.9415 - val_loss: 1.0069 - val_accuracy: 0.7260\n",
            "Epoch 11/50\n",
            "40/40 [==============================] - 6s 141ms/step - loss: 0.9249 - accuracy: 0.9415 - val_loss: 1.0062 - val_accuracy: 0.7258\n",
            "Epoch 12/50\n",
            "40/40 [==============================] - 6s 141ms/step - loss: 0.9236 - accuracy: 0.9447 - val_loss: 1.0087 - val_accuracy: 0.7183\n",
            "Epoch 13/50\n",
            "40/40 [==============================] - 6s 141ms/step - loss: 0.9218 - accuracy: 0.9470 - val_loss: 1.0059 - val_accuracy: 0.7276\n",
            "Epoch 14/50\n",
            "40/40 [==============================] - 6s 143ms/step - loss: 0.9204 - accuracy: 0.9518 - val_loss: 1.0050 - val_accuracy: 0.7301\n",
            "Epoch 15/50\n",
            "40/40 [==============================] - 6s 142ms/step - loss: 0.9199 - accuracy: 0.9542 - val_loss: 1.0059 - val_accuracy: 0.7271\n",
            "Epoch 16/50\n",
            "40/40 [==============================] - 6s 142ms/step - loss: 0.9196 - accuracy: 0.9565 - val_loss: 1.0065 - val_accuracy: 0.7259\n",
            "Epoch 17/50\n",
            "40/40 [==============================] - 6s 142ms/step - loss: 0.9187 - accuracy: 0.9581 - val_loss: 1.0078 - val_accuracy: 0.7217\n",
            "Epoch 18/50\n",
            "40/40 [==============================] - 6s 142ms/step - loss: 0.9188 - accuracy: 0.9573 - val_loss: 1.0064 - val_accuracy: 0.7256\n",
            "Epoch 19/50\n",
            "40/40 [==============================] - 6s 142ms/step - loss: 0.9185 - accuracy: 0.9581 - val_loss: 1.0091 - val_accuracy: 0.7214\n",
            "Epoch 20/50\n",
            "40/40 [==============================] - 6s 142ms/step - loss: 0.9185 - accuracy: 0.9589 - val_loss: 1.0119 - val_accuracy: 0.7121\n",
            "Epoch 21/50\n",
            "40/40 [==============================] - 6s 143ms/step - loss: 0.9179 - accuracy: 0.9605 - val_loss: 1.0070 - val_accuracy: 0.7237\n",
            "Epoch 22/50\n",
            "40/40 [==============================] - 6s 144ms/step - loss: 0.9168 - accuracy: 0.9613 - val_loss: 1.0078 - val_accuracy: 0.7227\n",
            "Epoch 23/50\n",
            "40/40 [==============================] - 6s 143ms/step - loss: 0.9159 - accuracy: 0.9636 - val_loss: 1.0100 - val_accuracy: 0.7190\n",
            "Epoch 24/50\n",
            "40/40 [==============================] - 6s 142ms/step - loss: 0.9167 - accuracy: 0.9621 - val_loss: 1.0099 - val_accuracy: 0.7176\n",
            "Epoch 25/50\n",
            "40/40 [==============================] - 6s 142ms/step - loss: 0.9158 - accuracy: 0.9628 - val_loss: 1.0104 - val_accuracy: 0.7152\n",
            "Epoch 26/50\n",
            "40/40 [==============================] - 6s 142ms/step - loss: 0.9170 - accuracy: 0.9605 - val_loss: 1.0136 - val_accuracy: 0.7077\n",
            "Epoch 27/50\n",
            "40/40 [==============================] - 6s 142ms/step - loss: 0.9160 - accuracy: 0.9621 - val_loss: 1.0105 - val_accuracy: 0.7162\n",
            "Epoch 28/50\n",
            "40/40 [==============================] - 6s 143ms/step - loss: 0.9161 - accuracy: 0.9636 - val_loss: 1.0176 - val_accuracy: 0.6988\n",
            "Epoch 29/50\n",
            "40/40 [==============================] - 6s 142ms/step - loss: 0.9140 - accuracy: 0.9708 - val_loss: 1.0115 - val_accuracy: 0.7127\n",
            "Epoch 30/50\n",
            "40/40 [==============================] - 6s 141ms/step - loss: 0.9135 - accuracy: 0.9708 - val_loss: 1.0120 - val_accuracy: 0.7123\n",
            "Epoch 31/50\n",
            "40/40 [==============================] - 6s 141ms/step - loss: 0.9128 - accuracy: 0.9747 - val_loss: 1.0124 - val_accuracy: 0.7124\n",
            "Epoch 32/50\n",
            "40/40 [==============================] - 6s 142ms/step - loss: 0.9118 - accuracy: 0.9763 - val_loss: 1.0082 - val_accuracy: 0.7215\n",
            "Epoch 33/50\n",
            "40/40 [==============================] - 6s 142ms/step - loss: 0.9132 - accuracy: 0.9715 - val_loss: 1.0115 - val_accuracy: 0.7137\n",
            "Epoch 34/50\n",
            "40/40 [==============================] - 6s 142ms/step - loss: 0.9130 - accuracy: 0.9747 - val_loss: 1.0111 - val_accuracy: 0.7160\n",
            "Epoch 35/50\n",
            "40/40 [==============================] - 6s 142ms/step - loss: 0.9116 - accuracy: 0.9779 - val_loss: 1.0132 - val_accuracy: 0.7108\n",
            "Epoch 36/50\n",
            "40/40 [==============================] - 6s 143ms/step - loss: 0.9102 - accuracy: 0.9794 - val_loss: 1.0109 - val_accuracy: 0.7161\n",
            "Epoch 37/50\n",
            "40/40 [==============================] - 6s 142ms/step - loss: 0.9094 - accuracy: 0.9818 - val_loss: 1.0265 - val_accuracy: 0.6750\n",
            "Epoch 38/50\n",
            "40/40 [==============================] - 6s 143ms/step - loss: 0.9096 - accuracy: 0.9802 - val_loss: 1.0143 - val_accuracy: 0.7062\n",
            "Epoch 39/50\n",
            "40/40 [==============================] - 6s 142ms/step - loss: 0.9105 - accuracy: 0.9779 - val_loss: 1.0184 - val_accuracy: 0.6973\n",
            "Epoch 40/50\n",
            "40/40 [==============================] - 6s 142ms/step - loss: 0.9106 - accuracy: 0.9787 - val_loss: 1.0287 - val_accuracy: 0.6690\n",
            "Epoch 41/50\n",
            "40/40 [==============================] - 6s 142ms/step - loss: 0.9110 - accuracy: 0.9763 - val_loss: 1.0159 - val_accuracy: 0.7023\n",
            "Epoch 42/50\n",
            "40/40 [==============================] - 6s 142ms/step - loss: 0.9124 - accuracy: 0.9771 - val_loss: 1.0226 - val_accuracy: 0.6858\n",
            "Epoch 43/50\n",
            "40/40 [==============================] - 6s 142ms/step - loss: 0.9124 - accuracy: 0.9771 - val_loss: 1.0204 - val_accuracy: 0.6912\n",
            "Epoch 44/50\n",
            "40/40 [==============================] - 6s 142ms/step - loss: 0.9104 - accuracy: 0.9810 - val_loss: 1.0150 - val_accuracy: 0.7055\n",
            "Epoch 45/50\n",
            "40/40 [==============================] - 7s 177ms/step - loss: 0.9084 - accuracy: 0.9858 - val_loss: 1.0133 - val_accuracy: 0.7076\n",
            "Epoch 46/50\n",
            "40/40 [==============================] - 6s 142ms/step - loss: 0.9080 - accuracy: 0.9858 - val_loss: 1.0126 - val_accuracy: 0.7109\n",
            "Epoch 47/50\n",
            "40/40 [==============================] - 6s 142ms/step - loss: 0.9082 - accuracy: 0.9842 - val_loss: 1.0106 - val_accuracy: 0.7160\n",
            "Epoch 48/50\n",
            "40/40 [==============================] - 6s 142ms/step - loss: 0.9078 - accuracy: 0.9850 - val_loss: 1.0137 - val_accuracy: 0.7059\n",
            "Epoch 49/50\n",
            "40/40 [==============================] - 6s 143ms/step - loss: 0.9069 - accuracy: 0.9881 - val_loss: 1.0121 - val_accuracy: 0.7124\n",
            "Epoch 50/50\n",
            "40/40 [==============================] - 6s 142ms/step - loss: 0.9071 - accuracy: 0.9874 - val_loss: 1.0206 - val_accuracy: 0.6888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU5ART7s7zNT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69c2d3ae-32d8-4626-dd4b-666334a191b1"
      },
      "source": [
        "loss, accuracy = model.evaluate(X_test, Y_test)\n",
        "print('Loss = ' + str(loss))\n",
        "print('Test Accuracy = ' + str(accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 4s 14ms/step - loss: 1.0206 - accuracy: 0.6888\n",
            "Loss = 1.020601749420166\n",
            "Test Accuracy = 0.6887999773025513\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwGpZ2mQ8Fwd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2962332-6c4e-4dd3-d778-859f61b2d823"
      },
      "source": [
        "#!pip install -q tensorflow==2.0.0b1\n",
        "# Install bleeding edge version of cleverhans\n",
        "!pip install git+https://github.com/tensorflow/cleverhans.git#egg=cleverhans\n",
        "\n",
        "import cleverhans\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cleverhans\n",
            "  Cloning https://github.com/tensorflow/cleverhans.git to /tmp/pip-install-voyyymfb/cleverhans_0b1277c443ed4136bb320ab41a76a31e\n",
            "  Running command git clone -q https://github.com/tensorflow/cleverhans.git /tmp/pip-install-voyyymfb/cleverhans_0b1277c443ed4136bb320ab41a76a31e\n",
            "Collecting nose\n",
            "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[K     |████████████████████████████████| 154 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting pycodestyle\n",
            "  Downloading pycodestyle-2.8.0-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 885 kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from cleverhans) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from cleverhans) (3.2.2)\n",
            "Collecting mnist\n",
            "  Downloading mnist-0.2.2-py2.py3-none-any.whl (3.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from cleverhans) (1.19.5)\n",
            "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.7/dist-packages (from cleverhans) (0.14.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from cleverhans) (1.0.1)\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.7/dist-packages (from cleverhans) (1.9)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from cleverhans) (0.12.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cleverhans) (1.15.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->cleverhans) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->cleverhans) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->cleverhans) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->cleverhans) (2.4.7)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability->cleverhans) (0.1.6)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability->cleverhans) (1.3.0)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability->cleverhans) (0.4.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability->cleverhans) (4.4.2)\n",
            "Building wheels for collected packages: cleverhans\n",
            "  Building wheel for cleverhans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cleverhans: filename=cleverhans-4.0.0-py3-none-any.whl size=92423 sha256=0f2c70ba72516dc16c22d1fb725869cfa00046a7a66781ee5454872bcc04a4fd\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_hl8_yjp/wheels/60/54/1e/97e3fe32d62bd252c9fbbee44a0545028c6018b81c054af3e4\n",
            "Successfully built cleverhans\n",
            "Installing collected packages: pycodestyle, nose, mnist, cleverhans\n",
            "Successfully installed cleverhans-4.0.0 mnist-0.2.2 nose-1.3.7 pycodestyle-2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEXj_jju8FzF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f532ea42-a948-43d4-b8ba-30fd658f7709"
      },
      "source": [
        "# Import the attack\n",
        "from cleverhans.tf2.attacks import fast_gradient_method\n",
        "\n",
        "#The attack requires the model to ouput the logits\n",
        "logits_model = tf.keras.Model(model.input,model.layers[-1].output)\n",
        "print(logits_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<keras.engine.functional.Functional object at 0x7fc4b6dcdf90>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElPwr6eS-1ME"
      },
      "source": [
        "def adv_generate(X_images):\n",
        "  images = []\n",
        "  labels = []\n",
        "  adv_orig_images = []\n",
        "  adv_orig_labels = []\n",
        "  epsilon = 0.1\n",
        "\n",
        "  for item in X_images:\n",
        "    #cv2_imshow(item)\n",
        "    adv_orig_images.append(item)\n",
        "    adv_orig_labels.append(0)\n",
        "\n",
        "    original_image = tf.convert_to_tensor(item.reshape((1,32,32,3))) \n",
        "    adv_img = fast_gradient_method.fast_gradient_method(logits_model, original_image, epsilon, np.inf, targeted=False)\n",
        "    adv_img = np.reshape(adv_img, (1,32,32,3))\n",
        "\n",
        "    adv_labels = model.predict(adv_img)\n",
        "    images.append(np.reshape(adv_img, (32,32,3)))\n",
        "    #print(adv_labels)\n",
        "    labels.append(np.reshape(adv_labels,(10)))\n",
        "\n",
        "    adv_orig_images.append(np.reshape(adv_img,(32,32,3)))\n",
        "    adv_orig_labels.append(1)\n",
        "\n",
        "  images = np.array(images)\n",
        "  labels = np.array(labels)\n",
        "  adv_orig_images = np.array(adv_orig_images)\n",
        "  adv_orig_labels = np.array(adv_orig_labels)\n",
        "  #labels = labels.reshape(-1, 1) \n",
        "  adv_orig_labels1 = adv_orig_labels.reshape(-1, 1) \n",
        "  adv_orig_images,adv_orig_labels1 = shuffle( adv_orig_images, adv_orig_labels, random_state=23)\n",
        "  return images, labels, adv_orig_images, adv_orig_labels\n",
        "  \n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaRJCGJUBlfq"
      },
      "source": [
        "\n",
        "x_train_adv, y_train_adv, X_train_adv_orig, Y_train_adv_orig = adv_generate(x_train[0:1265])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COx-PwYHQlUn"
      },
      "source": [
        "\n",
        "x_test_adv, y_test_adv, X_test_adv_orig, Y_test_adv_orig = adv_generate(x_test[0:565])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUCfYWXhpKNf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f1197b8-281f-47f7-d7c9-e73013e84e80"
      },
      "source": [
        "X_test_adv_orig[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[153., 163., 120.],\n",
              "        [173., 183., 140.],\n",
              "        [188., 197., 163.],\n",
              "        ...,\n",
              "        [106.,  92.,  68.],\n",
              "        [ 67.,  54.,  38.],\n",
              "        [ 61.,  49.,  36.]],\n",
              "\n",
              "       [[ 69.,  80.,  44.],\n",
              "        [119., 130.,  95.],\n",
              "        [182., 192., 159.],\n",
              "        ...,\n",
              "        [117.,  93.,  70.],\n",
              "        [104.,  83.,  67.],\n",
              "        [110.,  94.,  80.]],\n",
              "\n",
              "       [[ 50.,  55.,  41.],\n",
              "        [ 77.,  81.,  68.],\n",
              "        [174., 180., 162.],\n",
              "        ...,\n",
              "        [154., 131., 113.],\n",
              "        [142., 121., 107.],\n",
              "        [133., 115.,  98.]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[152., 135., 108.],\n",
              "        [146., 133., 106.],\n",
              "        [141., 133., 107.],\n",
              "        ...,\n",
              "        [152., 144., 127.],\n",
              "        [162., 152., 132.],\n",
              "        [126., 112.,  91.]],\n",
              "\n",
              "       [[147., 130., 113.],\n",
              "        [132., 121., 106.],\n",
              "        [125., 114.,  97.],\n",
              "        ...,\n",
              "        [174., 163., 139.],\n",
              "        [177., 168., 147.],\n",
              "        [140., 131., 109.]],\n",
              "\n",
              "       [[122., 111.,  92.],\n",
              "        [109.,  99.,  82.],\n",
              "        [158., 140., 118.],\n",
              "        ...,\n",
              "        [143., 129., 102.],\n",
              "        [132., 124., 101.],\n",
              "        [142., 135., 114.]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKXImWdRez2a"
      },
      "source": [
        "X_train_adv_orig = X_train_adv_orig.astype('uint8')\n",
        "#X_train_adv_orig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agu73YG2e9YN"
      },
      "source": [
        "X_test_adv_orig = X_test_adv_orig.astype('uint8')\n",
        "#X_test_adv_orig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6KuB8MWgQMl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66f2886f-dd99-4dfd-e58a-0f11bf8e6bc9"
      },
      "source": [
        "X_train_adv_orig.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4000, 32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf9DCPGQh9w5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7be57c0-22eb-4793-aa58-da2c4a574097"
      },
      "source": [
        "X_test_adv_orig.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1200, 32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBIhbq17TuN3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "902d81b7-e487-4a5a-a469-fdd8cd6e1e0e"
      },
      "source": [
        "Y_test_adv_orig.shape\n",
        "#X_test_adv_orig.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1200,)"
            ]
          },
          "metadata": {},
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6TvPB8YVHiR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d236a4df-7e57-4d67-ed95-e6f5ad7ba62f"
      },
      "source": [
        "\n",
        "Y_train_adv_orig"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, ..., 1, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvX6sSpahy_H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53c5763d-3661-4850-d1d5-d457f3bcab5c"
      },
      "source": [
        "n_classes1 = 2\n",
        "img_rows1, img_cols1, img_channel1 = 32, 32, 3\n",
        "\n",
        "# mnist is grey-scaled image, thus the last dimension, channel size will be 1\n",
        "X_train1_adv_orig = X_train_adv_orig.reshape(X_train_adv_orig.shape[0], img_rows1, img_cols1, img_channel1)\n",
        "X_test1_adv_orig  = X_test_adv_orig.reshape(X_test_adv_orig.shape[0], img_rows1, img_cols1, img_channel1)\n",
        "input_shape1 = img_rows1, img_cols1, img_channel1\n",
        "\n",
        "X_train1_adv_orig = X_train1_adv_orig.astype('float32')\n",
        "X_test1_adv_orig  = X_test1_adv_orig.astype('float32')\n",
        "\n",
        "# images takes values between 0 - 255, we can normalize it\n",
        "# by dividing every number by 255\n",
        "X_train1_adv_orig /= 255\n",
        "X_test1_adv_orig /= 255\n",
        "print('train shape:', X_train1_adv_orig.shape)\n",
        "\n",
        "# one-hot encode the class (target) vectors\n",
        "Y_train1_adv_orig = np_utils.to_categorical(Y_train_adv_orig, n_classes1)\n",
        "Y_test1_adv_orig = np_utils.to_categorical(Y_test_adv_orig , n_classes1)\n",
        "#print('Y_train shape:', Y_test1_adv_orig.shape)\n",
        "\n",
        "#Y_train\n",
        "#Y_train1_adv_orig[0].shape\n",
        "Y_train1_adv_orig.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train shape: (2530, 32, 32, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2530, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNaM77oERrXB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a45097d2-aa65-4345-9012-1f880c209942"
      },
      "source": [
        "Y_test_adv_orig.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1130,)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23_p88bjpheF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "4ef284b9-d018-4f62-80b1-e6a4df7093a1"
      },
      "source": [
        "cv2_imshow(x_test[5])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=32x32 at 0x7FC4B6300E50>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOWzjTwBSAA4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "551ab6ce-da13-4097-f3b2-441a932ecc03"
      },
      "source": [
        "cv2_imshow(np.reshape(x_test_adv[4], (32,32,3)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAJ3ElEQVR4nAXB2XLk1mEAUNwVwMXSQO/s5s4ZkrNqpMxIJbkSR4nKL8mzX/wT+bW8uMpKOS7ZKi+xFivOcDbOkGw2yd7RaKwXd0HOAZ989aXvkVU0W69WZcL9fthqYkgARIZIxNkPI+qTrfs7ANtEaCCV1QlbB1sYI6k0JVisNrPJHGhx+unnkte///rr4f7QJPbk+ta1ked42HJx2LE5d6No5fWaW0d9Xa6BYWBspGUutGr4Da/boTVW8SZFmrVdpYQqeCUUcExkGIILm2K/0YrTPN/ky9s5pTaqUeA7qqig38DUJJ7vUttpbvcczwYQEEoIJkWhDIgt15FSIKmzrNwkeS11GZeTi0kyW9VKAoqrjEOEiWkhbIq8rPOq6fl7+33LsuJZgrudxmqdMNdyQ6+5FRg8JZDY1NJalKUklNYSpJPIgFaScgUMCzHX8aDQAAiMkZAaIEgIQRBZpj3Y6e/tbHeHW4Dy8eU4KnKMsIkwGPS7C77BBPCYm5BCTeq6rqrKMHS0iB3HVlYZtALLc1WdlDI3mZIVyOMCUloTwFxmQ5N1/dNnpwgbtV1DBInNnn7xCY7jRGtwNbo2ibNZ5pZQBNL1OqMMCqkBgMSkwW6rETiuxyA0aqF4KkANVrN0OY9PXjwM+y2jNixiOn7gNx2hCm2IwA3NnTBNE5xniYbi73+7HOwNHcdjTmDwOotjIQ1Zacdxdz86aN1rQ4QgAPHVdPzqOvSaTx4/unr5XbpYNzynRlBx7gXMNS3LcdzarhXoBO0fXv797dkbXJS51JWseWvgmNrmlUIAWpYbrea8KPcfHw0/3q9qbkAQ3yb/98e3aZySE4cbqtv1DWhSaBqmsIfeis8812O2QzXW0oBCXb/9MHs/xcy1F4t0e9g/PNoP7fDi/ejqw63XaXKDNPpV/3QbEohKCCQYf/8hW2WHT08efHZ6PbqDpn/84pT4sBXYkJF1VbLVlBoAQISgKpJkOZvXWkOvaRNETUx9y2W+ffLg0MRwOZ1YzHvw9NmgvxtoP4Dh/NV8Ppp3DnqPPjt1W57XCZoPB80Tk4YmBmR5PgXKKmRRQ0ywQaGYzheLZYQggARbQBMlaq00gMBy7JNHR0pWNzevnT6sCddxbaWtwOjfP3705PlT0yFZngV7nfDAM0iBbHPyZnR39h5Bq4TIqLULlVQiF1oprWqNGeqV+aVUquJCSYVMuH+8fXV5pxcTe2Bu5FLF3YbyXDv81y/vNQfNuIhXIC0UV7dUZzq3UwTIycf3vLYVLZdI5Ii6EpkGsgCASZriJEqzNDMQ2ESxUnV/p2vZ8KPPHx+VTwRC8aIwaQ8olkbG+YfJECEGfUshLTiPKMM0ul147j3T4GVSEoyX2UbUnAV9LTSmuNMbYJOB3na/5FwKxctqPol29rvNVohXzu019+mwgEKAyh8MtBCza1GLOULadbyG7VCKKYTQ9OPlIr6srGZNKYM2IsSoNT882R/uHmAnsOiCWr6NMcUIT26j/lYXIMU3Mo9EpWamRSzXN4jle0zmZZlzpeokTUucEowYMsIWbTZ2Kq3Hb867vRARXhUpNhA27aquYJ5lopK84lLLWimb4WSTa609H//8n79ohezdy/8poykRKl0mpskODo76W0O/00aWXZt0EkebfNPv+YMwHIQHqxUkAMuyev/u8vXbC8hMXBSVyxxhCG1p17c7DpNKKaWX8Y3H7n/y5NOzH76XPG/YNqIWgHB6e0tMc39/D+gaEZKlO9Pbu7NX5x8/On7UPJr/ZZVHK2SIeLNsh42jo0Pwy//4FTS02/K55v1mezVdN9tOygWhVjnzX3z0s3W8vLwaY+I1Wy0DGKISQqucc5tSITWxMWU0W8ZNh2zitR90Lyd/pW558eHy4uzy2Yvn2LYZULLdDAWHZVWOb2ZGHSZCdrdsRVY//vTHX/zLvxVlff5+5NpmVfF2f4BNM03WJrWEAotoCk1lOfYmK3Iu3v34TZJfksC1W43tk+1er439ho1qRSC8uLoFTgWwHo+mvf3tsqi6w+bf/nT2h9879x9/nBaly6jXb8d5xauq1WwDoCe3t7BSoDIAkEoratqz2XW7BaPFar2WP/unT3rtPmaurcrk/M3FIso6zDGQyIrMImh0+SFZbe49GX77219vePLkyaei5BZjmND1Oi6LymW2TaC2TYxspSshuFT86HCH4LSGcacXmqaxLKfQxOZynr55fTXY27KsUCvSaLhVUTabrfHdyPbow384GN+dx8miM+iKWmslh/1umUZ3k9tCUaGNaBXxSkpdK1kDiy6zbPdg+/Hzh7lWSRnjzTpexylhrtSgNk0rbBaLO15lu0f7Yafx/t37o71ThmFVV1mZE+aXMilFFfhstl5EUdHwfEiYAtB2wlQleeaYZtDtNeZ8nsiU1gbcZDkx2Zdf/eLLr/59eO84PN0xekEmSymSltvAmrx++arZavlue7lMKm5wCZJko5U0NPQcx7aYljWFJgBwY+YasK2twxKXi3hhWkgbNW72m8f3t/aOnzXaIWn6xoICF8vpMtH67mrUYEG/Q5Ji5jg7tUS8VLySlqEwohBQKYt+d8uYzZIsTYo1qEu5LsbFvNeuRQVcxxQmxGVe3KTjqagODvbave2TwQmGaEVtxXmZcBFvjo+fEmYtZ2sLdxbz8XJ545C61z+0mYcQqKqCQpykqRCy6/beZWcXFy+9vQNCWVmI0dU1XkyWSsrR67Ob6cGLz79wg/Z2ew9BNFtfdx/sROPZX8/P+2FQ175RJOPR6PrqTbfVpazdCjp+EI7urm3mB82AZdlqM0+zVbKJATCULM4/TGpt46rImeXPL99NL0bZJn34xYswbO62+w3buYxGzrbOrPQ621iWBDrxOsUOxmkaGUjWtbGONr1eK06LOI4ohovlzcX5D4fP2hDQ8dsxZa6oKbaYXUkDKLicTL/9z9+ihn/vyX0fs463raE51m+MLVBzWvFaWKLb7m3J7maVVTxJarescmpj03TqOvowvji/fG0zY3vY/cs3//v8+c8//8cXX//3HzB0iCElC0k/2Ju8Gv/0X9/aPrMcBm3nsNFtMXK+uBL5RtnlPI7mVRLPSitnTS20tfZMqqtqlUZxelOTleUpu9W/upjrCrv3dpcYNdwA57rOluv5zd3+Zw/qrEqW6+9+8zsFpTyupBi0/Naj/kmcRIt8pgzEIAsoP/vx7Wx2d7S99f7DqigrYICga5883N3dDdMygxh2t1rSrrNkXWzWOJqu337/mmelZaFgp8WLavHu5ifjz8Qmq84mWPmH3UHbCyChFLA267D9jtfY++7P30yyi5ts0Q9ae7vDre3tncFOtFyURlobRuh5meZMGdWw+//5yqCIm3jUtAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=32x32 at 0x7FBA995DA550>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "Iy08yJ5N7vxI",
        "outputId": "aab98f64-d3a9-41de-89b5-84c85ba89f09"
      },
      "source": [
        "cv2_imshow(np.reshape(X_train_adv_orig[4], (32,32,3)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=32x32 at 0x7FBA1A5C1590>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmeG3Btk8J89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "1504f7fe-ee07-49af-eb88-d5976771731d"
      },
      "source": [
        "\n",
        "plt.imshow(np.reshape(x_test_adv[4], (32,32,3)).astype(int))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fba131f3a10>"
            ]
          },
          "metadata": {},
          "execution_count": 200
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVTZnQJu_TCQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1527b537-65f1-4212-ca9e-bc816201a708"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "Y_train_adv_orig.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4000,)"
            ]
          },
          "metadata": {},
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYlZc5xH02Ue"
      },
      "source": [
        "cv2_imshow(np.reshape(x_train_adv[4], (32,32,3)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0cIIiPQDV77"
      },
      "source": [
        "x_train_adv.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrxvYhSI_dWs"
      },
      "source": [
        "Y_train_adv_orig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOJ8VP-JTQJz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a52e9647-08f8-4e1e-99ac-5693351b9986"
      },
      "source": [
        "#X_train_adv_orig.shape[0]\n",
        "X_train_adv_orig.shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2530"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJk2dLFnKJDW"
      },
      "source": [
        "\n",
        "def ResNet1(input_shape1, n_classes1):\n",
        "    \"\"\"\n",
        "    Definition of ResNet\n",
        "    \n",
        "    References\n",
        "    ----------\n",
        "    https://github.com/keras-team/keras-applications/blob/master/keras_applications/resnet50.py\n",
        "    \"\"\"\n",
        "    img_input1 = layers.Input(shape=input_shape1)\n",
        "    \n",
        "    bn_axis = 3\n",
        "    \n",
        "    x = layers.ZeroPadding2D(padding=(3, 3), name='conv1_pad')(img_input1)\n",
        "    x = layers.Conv2D(64, (7, 7),\n",
        "                      strides=(2, 2),\n",
        "                      padding='valid',\n",
        "                      kernel_initializer='he_normal',\n",
        "                      name='conv1')(x)\n",
        "    x = layers.BatchNormalization(axis=bn_axis, name='bn_conv1')(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.ZeroPadding2D(padding=(1, 1), name='pool1_pad')(x)\n",
        "    x = layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
        "\n",
        "    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))\n",
        "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\n",
        "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')\n",
        "\n",
        "    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')\n",
        "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b')\n",
        "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c')\n",
        "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='d')\n",
        "    \n",
        "    # the commented out blocks are what's needed to build out the\n",
        "    # full ResNet50 (a ResNet with 50 layers), we won't be needing\n",
        "    # the complexity here\n",
        "    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')\n",
        "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b')\n",
        "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='c')\n",
        "    # x = identity_block(x, 3, [256, 256, 1024], stage=4, block='d')\n",
        "    # x = identity_block(x, 3, [256, 256, 1024], stage=4, block='e')\n",
        "    # x = identity_block(x, 3, [256, 256, 1024], stage=4, block='f')\n",
        "\n",
        "    # x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a')\n",
        "    # x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\n",
        "    # x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')\n",
        "    \n",
        "    x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n",
        "    #img_output1 = layers.Dense(n_classes, activation='softmax', name='fc' + str(n_classes))(x)\n",
        "    #img_output1 = layers.Dense(n_classes1, activation='softmax', name='fc' + str(n_classes1))(x)\n",
        "             #='linear' , name = 'fc' + str(n_classes))(x); \n",
        "    img_output1 = layers.Dense(n_classes1, kernel_regularizer=tensorflow.keras.regularizers.l2(0.01),activation ='softmax' , name = 'fc' + str(n_classes1))(x); \n",
        "    #model = Model(inputs=img_input, outputs=img_output, name='resnet')\n",
        "    model1 = Model(inputs=img_input1, outputs=img_output1, name='resnet1')\n",
        "    return model1\n",
        "\n",
        "model1 = ResNet1(input_shape1, n_classes1)\n",
        "model1.compile(optimizer = 'adam', loss ='binary_crossentropy', metrics = ['accuracy'])\n",
        "#model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "#model1.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8B780KDnOQ7b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_x86jEIOa0w"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQkA5pa6yYiQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cc0fbf5-c73d-4e71-c977-12744a0cc424"
      },
      "source": [
        "Y_train_adv_orig.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2530,)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMzQXVN_yljg"
      },
      "source": [
        "len(Y_train_adv_orig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tVZ_c4nGGHT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kVlmL9TQOBg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b62e8f35-d861-487d-9459-dc4b4581e4ff"
      },
      "source": [
        "history1 = model1.fit(X_train1_adv_orig, Y_train1_adv_orig, epochs=400, batch_size=32, validation_data=(X_test1_adv_orig, Y_test1_adv_orig))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "80/80 [==============================] - 9s 62ms/step - loss: 0.9155 - accuracy: 0.4937 - val_loss: 0.7283 - val_accuracy: 0.5000\n",
            "Epoch 2/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.8190 - accuracy: 0.5138 - val_loss: 0.7728 - val_accuracy: 0.4894\n",
            "Epoch 3/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.7822 - accuracy: 0.5229 - val_loss: 0.7301 - val_accuracy: 0.5000\n",
            "Epoch 4/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.7521 - accuracy: 0.5285 - val_loss: 0.7519 - val_accuracy: 0.4965\n",
            "Epoch 5/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.7418 - accuracy: 0.5482 - val_loss: 0.8054 - val_accuracy: 0.4912\n",
            "Epoch 6/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.7312 - accuracy: 0.5854 - val_loss: 0.9771 - val_accuracy: 0.4947\n",
            "Epoch 7/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.7050 - accuracy: 0.5953 - val_loss: 0.7669 - val_accuracy: 0.4965\n",
            "Epoch 8/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.7227 - accuracy: 0.5814 - val_loss: 3.1727 - val_accuracy: 0.4929\n",
            "Epoch 9/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.6962 - accuracy: 0.6059 - val_loss: 0.8968 - val_accuracy: 0.4735\n",
            "Epoch 10/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.6891 - accuracy: 0.6285 - val_loss: 1.1019 - val_accuracy: 0.5177\n",
            "Epoch 11/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.6571 - accuracy: 0.6466 - val_loss: 0.7543 - val_accuracy: 0.4912\n",
            "Epoch 12/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.6558 - accuracy: 0.6538 - val_loss: 6.2333 - val_accuracy: 0.4894\n",
            "Epoch 13/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.6491 - accuracy: 0.6605 - val_loss: 0.9006 - val_accuracy: 0.4858\n",
            "Epoch 14/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.6464 - accuracy: 0.6597 - val_loss: 1.5221 - val_accuracy: 0.4876\n",
            "Epoch 15/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.6497 - accuracy: 0.6506 - val_loss: 1.6295 - val_accuracy: 0.4717\n",
            "Epoch 16/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.6101 - accuracy: 0.6751 - val_loss: 1.4800 - val_accuracy: 0.4876\n",
            "Epoch 17/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.6073 - accuracy: 0.6644 - val_loss: 0.7526 - val_accuracy: 0.4982\n",
            "Epoch 18/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.5488 - accuracy: 0.6885 - val_loss: 1.0719 - val_accuracy: 0.4717\n",
            "Epoch 19/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.5480 - accuracy: 0.6901 - val_loss: 0.7416 - val_accuracy: 0.4805\n",
            "Epoch 20/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.5409 - accuracy: 0.6909 - val_loss: 1.5125 - val_accuracy: 0.4876\n",
            "Epoch 21/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.5368 - accuracy: 0.6854 - val_loss: 0.8465 - val_accuracy: 0.5071\n",
            "Epoch 22/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.5557 - accuracy: 0.6870 - val_loss: 0.8811 - val_accuracy: 0.4929\n",
            "Epoch 23/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.5148 - accuracy: 0.7004 - val_loss: 1.9464 - val_accuracy: 0.5177\n",
            "Epoch 24/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.5242 - accuracy: 0.6964 - val_loss: 0.8273 - val_accuracy: 0.4982\n",
            "Epoch 25/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.4755 - accuracy: 0.7111 - val_loss: 1.2632 - val_accuracy: 0.5000\n",
            "Epoch 26/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.5238 - accuracy: 0.7063 - val_loss: 0.7851 - val_accuracy: 0.5018\n",
            "Epoch 27/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.4528 - accuracy: 0.7300 - val_loss: 0.8844 - val_accuracy: 0.4947\n",
            "Epoch 28/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.5708 - accuracy: 0.6739 - val_loss: 1.7484 - val_accuracy: 0.5053\n",
            "Epoch 29/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.5002 - accuracy: 0.6972 - val_loss: 0.9671 - val_accuracy: 0.4929\n",
            "Epoch 30/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.4544 - accuracy: 0.7300 - val_loss: 0.8822 - val_accuracy: 0.5159\n",
            "Epoch 31/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.4495 - accuracy: 0.7138 - val_loss: 0.9393 - val_accuracy: 0.5159\n",
            "Epoch 32/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.4513 - accuracy: 0.7221 - val_loss: 0.8060 - val_accuracy: 0.5035\n",
            "Epoch 33/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.4465 - accuracy: 0.7257 - val_loss: 0.7739 - val_accuracy: 0.4894\n",
            "Epoch 34/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.4404 - accuracy: 0.7233 - val_loss: 0.9232 - val_accuracy: 0.4929\n",
            "Epoch 35/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.4462 - accuracy: 0.7209 - val_loss: 0.8916 - val_accuracy: 0.4805\n",
            "Epoch 36/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.4888 - accuracy: 0.7138 - val_loss: 0.8060 - val_accuracy: 0.4982\n",
            "Epoch 37/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.4520 - accuracy: 0.7162 - val_loss: 1.0199 - val_accuracy: 0.5071\n",
            "Epoch 38/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.4121 - accuracy: 0.7277 - val_loss: 0.8365 - val_accuracy: 0.4947\n",
            "Epoch 39/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.4173 - accuracy: 0.7296 - val_loss: 0.7752 - val_accuracy: 0.4752\n",
            "Epoch 40/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.4124 - accuracy: 0.7340 - val_loss: 0.8214 - val_accuracy: 0.4876\n",
            "Epoch 41/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.4501 - accuracy: 0.7257 - val_loss: 0.8855 - val_accuracy: 0.4894\n",
            "Epoch 42/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.4190 - accuracy: 0.7308 - val_loss: 0.7872 - val_accuracy: 0.4929\n",
            "Epoch 43/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.3944 - accuracy: 0.7356 - val_loss: 0.9849 - val_accuracy: 0.5000\n",
            "Epoch 44/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3929 - accuracy: 0.7419 - val_loss: 0.8165 - val_accuracy: 0.4894\n",
            "Epoch 45/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.4109 - accuracy: 0.7261 - val_loss: 0.8767 - val_accuracy: 0.5088\n",
            "Epoch 46/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.3954 - accuracy: 0.7336 - val_loss: 0.8884 - val_accuracy: 0.4947\n",
            "Epoch 47/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.4367 - accuracy: 0.7154 - val_loss: 0.9191 - val_accuracy: 0.4929\n",
            "Epoch 48/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.4110 - accuracy: 0.7221 - val_loss: 0.8818 - val_accuracy: 0.5000\n",
            "Epoch 49/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.3763 - accuracy: 0.7372 - val_loss: 0.7594 - val_accuracy: 0.5071\n",
            "Epoch 50/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.4317 - accuracy: 0.7281 - val_loss: 0.8713 - val_accuracy: 0.4912\n",
            "Epoch 51/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.4276 - accuracy: 0.7253 - val_loss: 0.7965 - val_accuracy: 0.4982\n",
            "Epoch 52/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.5998 - accuracy: 0.6542 - val_loss: 0.7472 - val_accuracy: 0.4965\n",
            "Epoch 53/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.5369 - accuracy: 0.6842 - val_loss: 0.7482 - val_accuracy: 0.4929\n",
            "Epoch 54/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.4951 - accuracy: 0.6937 - val_loss: 0.8826 - val_accuracy: 0.5053\n",
            "Epoch 55/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.4323 - accuracy: 0.7206 - val_loss: 0.9236 - val_accuracy: 0.5035\n",
            "Epoch 56/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.4747 - accuracy: 0.7115 - val_loss: 0.8561 - val_accuracy: 0.4858\n",
            "Epoch 57/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.4084 - accuracy: 0.7304 - val_loss: 0.8807 - val_accuracy: 0.4894\n",
            "Epoch 58/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.3915 - accuracy: 0.7435 - val_loss: 0.8727 - val_accuracy: 0.4646\n",
            "Epoch 59/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.4879 - accuracy: 0.6972 - val_loss: 0.7441 - val_accuracy: 0.5000\n",
            "Epoch 60/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.5041 - accuracy: 0.7051 - val_loss: 0.8180 - val_accuracy: 0.4770\n",
            "Epoch 61/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.4080 - accuracy: 0.7312 - val_loss: 0.9931 - val_accuracy: 0.4858\n",
            "Epoch 62/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3878 - accuracy: 0.7249 - val_loss: 0.9587 - val_accuracy: 0.4894\n",
            "Epoch 63/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.4145 - accuracy: 0.7241 - val_loss: 0.9981 - val_accuracy: 0.4858\n",
            "Epoch 64/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3829 - accuracy: 0.7352 - val_loss: 0.9748 - val_accuracy: 0.4841\n",
            "Epoch 65/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3764 - accuracy: 0.7340 - val_loss: 1.0067 - val_accuracy: 0.4929\n",
            "Epoch 66/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3690 - accuracy: 0.7490 - val_loss: 0.8600 - val_accuracy: 0.4805\n",
            "Epoch 67/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.3678 - accuracy: 0.7498 - val_loss: 1.0562 - val_accuracy: 0.4788\n",
            "Epoch 68/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.6596 - accuracy: 0.6221 - val_loss: 0.8799 - val_accuracy: 0.4788\n",
            "Epoch 69/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.5964 - accuracy: 0.6644 - val_loss: 0.8398 - val_accuracy: 0.5195\n",
            "Epoch 70/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.5552 - accuracy: 0.6822 - val_loss: 0.8532 - val_accuracy: 0.5071\n",
            "Epoch 71/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.4754 - accuracy: 0.7111 - val_loss: 0.7864 - val_accuracy: 0.4912\n",
            "Epoch 72/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.4182 - accuracy: 0.7300 - val_loss: 0.9087 - val_accuracy: 0.5035\n",
            "Epoch 73/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.4122 - accuracy: 0.7273 - val_loss: 0.9785 - val_accuracy: 0.4823\n",
            "Epoch 74/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.3990 - accuracy: 0.7285 - val_loss: 0.9807 - val_accuracy: 0.4717\n",
            "Epoch 75/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.3894 - accuracy: 0.7478 - val_loss: 0.9117 - val_accuracy: 0.4805\n",
            "Epoch 76/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.3725 - accuracy: 0.7360 - val_loss: 1.0297 - val_accuracy: 0.4947\n",
            "Epoch 77/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3999 - accuracy: 0.7364 - val_loss: 1.0595 - val_accuracy: 0.4947\n",
            "Epoch 78/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3982 - accuracy: 0.7391 - val_loss: 0.9366 - val_accuracy: 0.4912\n",
            "Epoch 79/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3770 - accuracy: 0.7375 - val_loss: 1.0138 - val_accuracy: 0.4982\n",
            "Epoch 80/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3709 - accuracy: 0.7427 - val_loss: 0.9763 - val_accuracy: 0.4947\n",
            "Epoch 81/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3786 - accuracy: 0.7447 - val_loss: 0.9945 - val_accuracy: 0.4664\n",
            "Epoch 82/400\n",
            "80/80 [==============================] - 4s 51ms/step - loss: 0.3705 - accuracy: 0.7470 - val_loss: 0.9144 - val_accuracy: 0.4965\n",
            "Epoch 83/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.4118 - accuracy: 0.7352 - val_loss: 0.8538 - val_accuracy: 0.4947\n",
            "Epoch 84/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3872 - accuracy: 0.7320 - val_loss: 0.8814 - val_accuracy: 0.5035\n",
            "Epoch 85/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.5984 - accuracy: 0.6542 - val_loss: 0.7206 - val_accuracy: 0.4929\n",
            "Epoch 86/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.4963 - accuracy: 0.7154 - val_loss: 0.8264 - val_accuracy: 0.5088\n",
            "Epoch 87/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.5076 - accuracy: 0.6996 - val_loss: 0.9082 - val_accuracy: 0.4823\n",
            "Epoch 88/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.4317 - accuracy: 0.7257 - val_loss: 0.8116 - val_accuracy: 0.4965\n",
            "Epoch 89/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3983 - accuracy: 0.7431 - val_loss: 0.9049 - val_accuracy: 0.4929\n",
            "Epoch 90/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.4061 - accuracy: 0.7344 - val_loss: 0.9711 - val_accuracy: 0.4858\n",
            "Epoch 91/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3883 - accuracy: 0.7395 - val_loss: 1.0687 - val_accuracy: 0.4770\n",
            "Epoch 92/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3830 - accuracy: 0.7431 - val_loss: 0.9228 - val_accuracy: 0.4876\n",
            "Epoch 93/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3961 - accuracy: 0.7411 - val_loss: 0.9441 - val_accuracy: 0.4947\n",
            "Epoch 94/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3952 - accuracy: 0.7308 - val_loss: 0.9232 - val_accuracy: 0.4805\n",
            "Epoch 95/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3777 - accuracy: 0.7427 - val_loss: 1.0169 - val_accuracy: 0.5018\n",
            "Epoch 96/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.5521 - accuracy: 0.6759 - val_loss: 0.8667 - val_accuracy: 0.4965\n",
            "Epoch 97/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.4497 - accuracy: 0.7277 - val_loss: 0.8314 - val_accuracy: 0.5035\n",
            "Epoch 98/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.4066 - accuracy: 0.7360 - val_loss: 0.9290 - val_accuracy: 0.5000\n",
            "Epoch 99/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3953 - accuracy: 0.7372 - val_loss: 0.9244 - val_accuracy: 0.4929\n",
            "Epoch 100/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3793 - accuracy: 0.7399 - val_loss: 0.9641 - val_accuracy: 0.4912\n",
            "Epoch 101/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3716 - accuracy: 0.7470 - val_loss: 0.9466 - val_accuracy: 0.4982\n",
            "Epoch 102/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.4399 - accuracy: 0.7308 - val_loss: 1.0796 - val_accuracy: 0.4912\n",
            "Epoch 103/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.4026 - accuracy: 0.7419 - val_loss: 0.8726 - val_accuracy: 0.5035\n",
            "Epoch 104/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3825 - accuracy: 0.7423 - val_loss: 0.9477 - val_accuracy: 0.5159\n",
            "Epoch 105/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3716 - accuracy: 0.7419 - val_loss: 1.0075 - val_accuracy: 0.4929\n",
            "Epoch 106/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3679 - accuracy: 0.7415 - val_loss: 1.0604 - val_accuracy: 0.4929\n",
            "Epoch 107/400\n",
            "80/80 [==============================] - 4s 54ms/step - loss: 0.3630 - accuracy: 0.7419 - val_loss: 1.0394 - val_accuracy: 0.4858\n",
            "Epoch 108/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3710 - accuracy: 0.7494 - val_loss: 1.1162 - val_accuracy: 0.4965\n",
            "Epoch 109/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3735 - accuracy: 0.7344 - val_loss: 1.0522 - val_accuracy: 0.4947\n",
            "Epoch 110/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3669 - accuracy: 0.7360 - val_loss: 0.9992 - val_accuracy: 0.5018\n",
            "Epoch 111/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3685 - accuracy: 0.7470 - val_loss: 1.0731 - val_accuracy: 0.5035\n",
            "Epoch 112/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3638 - accuracy: 0.7458 - val_loss: 1.0477 - val_accuracy: 0.5018\n",
            "Epoch 113/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3620 - accuracy: 0.7451 - val_loss: 1.0621 - val_accuracy: 0.4894\n",
            "Epoch 114/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3586 - accuracy: 0.7403 - val_loss: 1.1073 - val_accuracy: 0.5035\n",
            "Epoch 115/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3583 - accuracy: 0.7435 - val_loss: 1.0724 - val_accuracy: 0.5124\n",
            "Epoch 116/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3591 - accuracy: 0.7415 - val_loss: 1.0155 - val_accuracy: 0.5000\n",
            "Epoch 117/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3564 - accuracy: 0.7257 - val_loss: 1.0590 - val_accuracy: 0.4982\n",
            "Epoch 118/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3616 - accuracy: 0.7431 - val_loss: 1.1141 - val_accuracy: 0.4947\n",
            "Epoch 119/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.4165 - accuracy: 0.7372 - val_loss: 0.8922 - val_accuracy: 0.4823\n",
            "Epoch 120/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3918 - accuracy: 0.7336 - val_loss: 1.0757 - val_accuracy: 0.4982\n",
            "Epoch 121/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3784 - accuracy: 0.7296 - val_loss: 1.1853 - val_accuracy: 0.4876\n",
            "Epoch 122/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3847 - accuracy: 0.7439 - val_loss: 1.0041 - val_accuracy: 0.4841\n",
            "Epoch 123/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3738 - accuracy: 0.7455 - val_loss: 0.9547 - val_accuracy: 0.4912\n",
            "Epoch 124/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3612 - accuracy: 0.7427 - val_loss: 0.9449 - val_accuracy: 0.5053\n",
            "Epoch 125/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3719 - accuracy: 0.7462 - val_loss: 0.8898 - val_accuracy: 0.4965\n",
            "Epoch 126/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3822 - accuracy: 0.7411 - val_loss: 0.9691 - val_accuracy: 0.4912\n",
            "Epoch 127/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3690 - accuracy: 0.7423 - val_loss: 0.9829 - val_accuracy: 0.4770\n",
            "Epoch 128/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3899 - accuracy: 0.7336 - val_loss: 1.0491 - val_accuracy: 0.4858\n",
            "Epoch 129/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.5042 - accuracy: 0.7123 - val_loss: 0.8835 - val_accuracy: 0.4965\n",
            "Epoch 130/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.4340 - accuracy: 0.7261 - val_loss: 0.9983 - val_accuracy: 0.4522\n",
            "Epoch 131/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3941 - accuracy: 0.7344 - val_loss: 1.1757 - val_accuracy: 0.4982\n",
            "Epoch 132/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3792 - accuracy: 0.7387 - val_loss: 1.2621 - val_accuracy: 0.4947\n",
            "Epoch 133/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.4096 - accuracy: 0.7399 - val_loss: 1.1018 - val_accuracy: 0.4929\n",
            "Epoch 134/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3885 - accuracy: 0.7379 - val_loss: 1.0032 - val_accuracy: 0.5000\n",
            "Epoch 135/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3630 - accuracy: 0.7383 - val_loss: 1.0636 - val_accuracy: 0.4947\n",
            "Epoch 136/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3624 - accuracy: 0.7344 - val_loss: 1.0223 - val_accuracy: 0.5000\n",
            "Epoch 137/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3565 - accuracy: 0.7423 - val_loss: 1.0509 - val_accuracy: 0.4982\n",
            "Epoch 138/400\n",
            "80/80 [==============================] - 4s 54ms/step - loss: 0.3613 - accuracy: 0.7439 - val_loss: 1.0572 - val_accuracy: 0.5053\n",
            "Epoch 139/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.5005 - accuracy: 0.7233 - val_loss: 1.2190 - val_accuracy: 0.4894\n",
            "Epoch 140/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.4300 - accuracy: 0.7340 - val_loss: 0.8866 - val_accuracy: 0.5071\n",
            "Epoch 141/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3817 - accuracy: 0.7458 - val_loss: 0.9904 - val_accuracy: 0.4912\n",
            "Epoch 142/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3687 - accuracy: 0.7375 - val_loss: 1.0886 - val_accuracy: 0.5035\n",
            "Epoch 143/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3656 - accuracy: 0.7395 - val_loss: 0.9893 - val_accuracy: 0.4982\n",
            "Epoch 144/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3621 - accuracy: 0.7470 - val_loss: 1.0841 - val_accuracy: 0.5018\n",
            "Epoch 145/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3599 - accuracy: 0.7518 - val_loss: 1.0722 - val_accuracy: 0.5053\n",
            "Epoch 146/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3600 - accuracy: 0.7462 - val_loss: 1.0491 - val_accuracy: 0.5088\n",
            "Epoch 147/400\n",
            "80/80 [==============================] - 4s 54ms/step - loss: 0.3644 - accuracy: 0.7435 - val_loss: 0.9275 - val_accuracy: 0.5071\n",
            "Epoch 148/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.5687 - accuracy: 0.6783 - val_loss: 0.8672 - val_accuracy: 0.5000\n",
            "Epoch 149/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.4854 - accuracy: 0.7178 - val_loss: 0.9391 - val_accuracy: 0.4982\n",
            "Epoch 150/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.4143 - accuracy: 0.7336 - val_loss: 0.9102 - val_accuracy: 0.4929\n",
            "Epoch 151/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3805 - accuracy: 0.7336 - val_loss: 1.0328 - val_accuracy: 0.5000\n",
            "Epoch 152/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3760 - accuracy: 0.7427 - val_loss: 0.9687 - val_accuracy: 0.5124\n",
            "Epoch 153/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.5158 - accuracy: 0.6968 - val_loss: 0.9612 - val_accuracy: 0.4947\n",
            "Epoch 154/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.4488 - accuracy: 0.7202 - val_loss: 0.8622 - val_accuracy: 0.4965\n",
            "Epoch 155/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.4099 - accuracy: 0.7356 - val_loss: 1.0466 - val_accuracy: 0.4947\n",
            "Epoch 156/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3750 - accuracy: 0.7391 - val_loss: 1.0821 - val_accuracy: 0.5071\n",
            "Epoch 157/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3691 - accuracy: 0.7403 - val_loss: 0.9918 - val_accuracy: 0.5035\n",
            "Epoch 158/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.4076 - accuracy: 0.7364 - val_loss: 1.0617 - val_accuracy: 0.5230\n",
            "Epoch 159/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3797 - accuracy: 0.7320 - val_loss: 0.9723 - val_accuracy: 0.5000\n",
            "Epoch 160/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3682 - accuracy: 0.7474 - val_loss: 0.9365 - val_accuracy: 0.4876\n",
            "Epoch 161/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3647 - accuracy: 0.7458 - val_loss: 1.0125 - val_accuracy: 0.4929\n",
            "Epoch 162/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3625 - accuracy: 0.7462 - val_loss: 0.9982 - val_accuracy: 0.4982\n",
            "Epoch 163/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3589 - accuracy: 0.7447 - val_loss: 0.9527 - val_accuracy: 0.4912\n",
            "Epoch 164/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3582 - accuracy: 0.7498 - val_loss: 0.9895 - val_accuracy: 0.4965\n",
            "Epoch 165/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.4075 - accuracy: 0.7265 - val_loss: 0.9552 - val_accuracy: 0.5035\n",
            "Epoch 166/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.4532 - accuracy: 0.7257 - val_loss: 0.9310 - val_accuracy: 0.4965\n",
            "Epoch 167/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3875 - accuracy: 0.7431 - val_loss: 1.0319 - val_accuracy: 0.5053\n",
            "Epoch 168/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3707 - accuracy: 0.7423 - val_loss: 1.0256 - val_accuracy: 0.4947\n",
            "Epoch 169/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3647 - accuracy: 0.7415 - val_loss: 1.0653 - val_accuracy: 0.4965\n",
            "Epoch 170/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3646 - accuracy: 0.7443 - val_loss: 1.0138 - val_accuracy: 0.4929\n",
            "Epoch 171/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3625 - accuracy: 0.7451 - val_loss: 1.0300 - val_accuracy: 0.5035\n",
            "Epoch 172/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3595 - accuracy: 0.7356 - val_loss: 1.1041 - val_accuracy: 0.4858\n",
            "Epoch 173/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3600 - accuracy: 0.7443 - val_loss: 0.9912 - val_accuracy: 0.4858\n",
            "Epoch 174/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3625 - accuracy: 0.7451 - val_loss: 1.0396 - val_accuracy: 0.5018\n",
            "Epoch 175/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3599 - accuracy: 0.7451 - val_loss: 1.0252 - val_accuracy: 0.4982\n",
            "Epoch 176/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3607 - accuracy: 0.7557 - val_loss: 1.0271 - val_accuracy: 0.4894\n",
            "Epoch 177/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3593 - accuracy: 0.7379 - val_loss: 1.0384 - val_accuracy: 0.4894\n",
            "Epoch 178/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3588 - accuracy: 0.7443 - val_loss: 1.0320 - val_accuracy: 0.4929\n",
            "Epoch 179/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3620 - accuracy: 0.7443 - val_loss: 0.9592 - val_accuracy: 0.4876\n",
            "Epoch 180/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3583 - accuracy: 0.7494 - val_loss: 1.0241 - val_accuracy: 0.4929\n",
            "Epoch 181/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3613 - accuracy: 0.7344 - val_loss: 1.0370 - val_accuracy: 0.4947\n",
            "Epoch 182/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3587 - accuracy: 0.7478 - val_loss: 1.0745 - val_accuracy: 0.4965\n",
            "Epoch 183/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.5485 - accuracy: 0.7079 - val_loss: 0.7986 - val_accuracy: 0.5124\n",
            "Epoch 184/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.4482 - accuracy: 0.7249 - val_loss: 0.8639 - val_accuracy: 0.5212\n",
            "Epoch 185/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.4000 - accuracy: 0.7336 - val_loss: 0.8807 - val_accuracy: 0.5000\n",
            "Epoch 186/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.4652 - accuracy: 0.7257 - val_loss: 0.8696 - val_accuracy: 0.5177\n",
            "Epoch 187/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3916 - accuracy: 0.7447 - val_loss: 1.0203 - val_accuracy: 0.5088\n",
            "Epoch 188/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3750 - accuracy: 0.7423 - val_loss: 1.0205 - val_accuracy: 0.5053\n",
            "Epoch 189/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3822 - accuracy: 0.7352 - val_loss: 1.1193 - val_accuracy: 0.4965\n",
            "Epoch 190/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3698 - accuracy: 0.7447 - val_loss: 1.0879 - val_accuracy: 0.4982\n",
            "Epoch 191/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3610 - accuracy: 0.7439 - val_loss: 1.1201 - val_accuracy: 0.5071\n",
            "Epoch 192/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3574 - accuracy: 0.7470 - val_loss: 1.1376 - val_accuracy: 0.5142\n",
            "Epoch 193/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3571 - accuracy: 0.7447 - val_loss: 1.0829 - val_accuracy: 0.5018\n",
            "Epoch 194/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3563 - accuracy: 0.7502 - val_loss: 1.1887 - val_accuracy: 0.5177\n",
            "Epoch 195/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.4591 - accuracy: 0.7209 - val_loss: 1.0050 - val_accuracy: 0.4912\n",
            "Epoch 196/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3905 - accuracy: 0.7383 - val_loss: 1.0320 - val_accuracy: 0.5053\n",
            "Epoch 197/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3778 - accuracy: 0.7344 - val_loss: 1.1666 - val_accuracy: 0.4894\n",
            "Epoch 198/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.4197 - accuracy: 0.7304 - val_loss: 1.0920 - val_accuracy: 0.5000\n",
            "Epoch 199/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.4061 - accuracy: 0.7364 - val_loss: 1.2992 - val_accuracy: 0.4947\n",
            "Epoch 200/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3797 - accuracy: 0.7439 - val_loss: 1.1852 - val_accuracy: 0.5000\n",
            "Epoch 201/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3727 - accuracy: 0.7387 - val_loss: 1.1259 - val_accuracy: 0.4965\n",
            "Epoch 202/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3696 - accuracy: 0.7466 - val_loss: 1.0814 - val_accuracy: 0.4912\n",
            "Epoch 203/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.4212 - accuracy: 0.7312 - val_loss: 1.0843 - val_accuracy: 0.4646\n",
            "Epoch 204/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3875 - accuracy: 0.7375 - val_loss: 1.0067 - val_accuracy: 0.4858\n",
            "Epoch 205/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.4239 - accuracy: 0.7281 - val_loss: 1.0362 - val_accuracy: 0.4947\n",
            "Epoch 206/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3763 - accuracy: 0.7289 - val_loss: 1.1775 - val_accuracy: 0.4912\n",
            "Epoch 207/400\n",
            "80/80 [==============================] - 4s 54ms/step - loss: 0.3725 - accuracy: 0.7383 - val_loss: 1.1029 - val_accuracy: 0.4876\n",
            "Epoch 208/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.4214 - accuracy: 0.7368 - val_loss: 1.0746 - val_accuracy: 0.4965\n",
            "Epoch 209/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3796 - accuracy: 0.7419 - val_loss: 1.0863 - val_accuracy: 0.5018\n",
            "Epoch 210/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3671 - accuracy: 0.7455 - val_loss: 1.1398 - val_accuracy: 0.4947\n",
            "Epoch 211/400\n",
            "80/80 [==============================] - 4s 54ms/step - loss: 0.3622 - accuracy: 0.7447 - val_loss: 1.1563 - val_accuracy: 0.4894\n",
            "Epoch 212/400\n",
            "80/80 [==============================] - 4s 54ms/step - loss: 0.3739 - accuracy: 0.7455 - val_loss: 1.1215 - val_accuracy: 0.4823\n",
            "Epoch 213/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3776 - accuracy: 0.7455 - val_loss: 1.1894 - val_accuracy: 0.4823\n",
            "Epoch 214/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3660 - accuracy: 0.7451 - val_loss: 1.1076 - val_accuracy: 0.4912\n",
            "Epoch 215/400\n",
            "80/80 [==============================] - 4s 54ms/step - loss: 0.3615 - accuracy: 0.7427 - val_loss: 1.1462 - val_accuracy: 0.4858\n",
            "Epoch 216/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3660 - accuracy: 0.7415 - val_loss: 1.1585 - val_accuracy: 0.4982\n",
            "Epoch 217/400\n",
            "80/80 [==============================] - 4s 54ms/step - loss: 0.3644 - accuracy: 0.7447 - val_loss: 1.2162 - val_accuracy: 0.4841\n",
            "Epoch 218/400\n",
            "80/80 [==============================] - 4s 54ms/step - loss: 0.3610 - accuracy: 0.7435 - val_loss: 1.1638 - val_accuracy: 0.4965\n",
            "Epoch 219/400\n",
            "80/80 [==============================] - 4s 54ms/step - loss: 0.3576 - accuracy: 0.7427 - val_loss: 1.2323 - val_accuracy: 0.5035\n",
            "Epoch 220/400\n",
            "80/80 [==============================] - 5s 57ms/step - loss: 0.3590 - accuracy: 0.7474 - val_loss: 1.1521 - val_accuracy: 0.5018\n",
            "Epoch 221/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3573 - accuracy: 0.7383 - val_loss: 1.2373 - val_accuracy: 0.5035\n",
            "Epoch 222/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3570 - accuracy: 0.7435 - val_loss: 1.2388 - val_accuracy: 0.4982\n",
            "Epoch 223/400\n",
            "80/80 [==============================] - 4s 54ms/step - loss: 0.3577 - accuracy: 0.7379 - val_loss: 1.2252 - val_accuracy: 0.4858\n",
            "Epoch 224/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3614 - accuracy: 0.7419 - val_loss: 1.1654 - val_accuracy: 0.4823\n",
            "Epoch 225/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3793 - accuracy: 0.7447 - val_loss: 1.3807 - val_accuracy: 0.4858\n",
            "Epoch 226/400\n",
            "80/80 [==============================] - 4s 54ms/step - loss: 0.3707 - accuracy: 0.7383 - val_loss: 1.1550 - val_accuracy: 0.4965\n",
            "Epoch 227/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3678 - accuracy: 0.7474 - val_loss: 1.3067 - val_accuracy: 0.5000\n",
            "Epoch 228/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3620 - accuracy: 0.7514 - val_loss: 1.1629 - val_accuracy: 0.5018\n",
            "Epoch 229/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3624 - accuracy: 0.7332 - val_loss: 1.1294 - val_accuracy: 0.5000\n",
            "Epoch 230/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3564 - accuracy: 0.7478 - val_loss: 1.2170 - val_accuracy: 0.5018\n",
            "Epoch 231/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3568 - accuracy: 0.7447 - val_loss: 1.2160 - val_accuracy: 0.4858\n",
            "Epoch 232/400\n",
            "80/80 [==============================] - 4s 54ms/step - loss: 0.4043 - accuracy: 0.7277 - val_loss: 0.9786 - val_accuracy: 0.5142\n",
            "Epoch 233/400\n",
            "80/80 [==============================] - 4s 54ms/step - loss: 0.3819 - accuracy: 0.7265 - val_loss: 1.0596 - val_accuracy: 0.5018\n",
            "Epoch 234/400\n",
            "80/80 [==============================] - 4s 54ms/step - loss: 0.4347 - accuracy: 0.7249 - val_loss: 1.0636 - val_accuracy: 0.4912\n",
            "Epoch 235/400\n",
            "80/80 [==============================] - 4s 54ms/step - loss: 0.4097 - accuracy: 0.7336 - val_loss: 0.8295 - val_accuracy: 0.4982\n",
            "Epoch 236/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.4092 - accuracy: 0.7198 - val_loss: 1.0935 - val_accuracy: 0.5177\n",
            "Epoch 237/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3973 - accuracy: 0.7375 - val_loss: 0.9451 - val_accuracy: 0.5177\n",
            "Epoch 238/400\n",
            "80/80 [==============================] - 4s 54ms/step - loss: 0.3716 - accuracy: 0.7443 - val_loss: 0.9956 - val_accuracy: 0.5142\n",
            "Epoch 239/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3742 - accuracy: 0.7403 - val_loss: 1.1221 - val_accuracy: 0.5053\n",
            "Epoch 240/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.4397 - accuracy: 0.7233 - val_loss: 0.9873 - val_accuracy: 0.4841\n",
            "Epoch 241/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3797 - accuracy: 0.7427 - val_loss: 0.9576 - val_accuracy: 0.5000\n",
            "Epoch 242/400\n",
            "80/80 [==============================] - 4s 54ms/step - loss: 0.4272 - accuracy: 0.7387 - val_loss: 1.0093 - val_accuracy: 0.4876\n",
            "Epoch 243/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3883 - accuracy: 0.7403 - val_loss: 0.9997 - val_accuracy: 0.4982\n",
            "Epoch 244/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3796 - accuracy: 0.7415 - val_loss: 1.0499 - val_accuracy: 0.4947\n",
            "Epoch 245/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3680 - accuracy: 0.7419 - val_loss: 1.0000 - val_accuracy: 0.4929\n",
            "Epoch 246/400\n",
            "80/80 [==============================] - 4s 52ms/step - loss: 0.3640 - accuracy: 0.7451 - val_loss: 1.0896 - val_accuracy: 0.4947\n",
            "Epoch 247/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3708 - accuracy: 0.7316 - val_loss: 0.9945 - val_accuracy: 0.4894\n",
            "Epoch 248/400\n",
            "80/80 [==============================] - 4s 54ms/step - loss: 0.3611 - accuracy: 0.7466 - val_loss: 1.0927 - val_accuracy: 0.5212\n",
            "Epoch 249/400\n",
            "80/80 [==============================] - 4s 54ms/step - loss: 0.3585 - accuracy: 0.7443 - val_loss: 1.0895 - val_accuracy: 0.5212\n",
            "Epoch 250/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3664 - accuracy: 0.7439 - val_loss: 0.9779 - val_accuracy: 0.5265\n",
            "Epoch 251/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3624 - accuracy: 0.7352 - val_loss: 1.0041 - val_accuracy: 0.5035\n",
            "Epoch 252/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3567 - accuracy: 0.7375 - val_loss: 1.0743 - val_accuracy: 0.5071\n",
            "Epoch 253/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3565 - accuracy: 0.7395 - val_loss: 1.0834 - val_accuracy: 0.4876\n",
            "Epoch 254/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3571 - accuracy: 0.7455 - val_loss: 1.1276 - val_accuracy: 0.4912\n",
            "Epoch 255/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3812 - accuracy: 0.7435 - val_loss: 1.0295 - val_accuracy: 0.4929\n",
            "Epoch 256/400\n",
            "80/80 [==============================] - 4s 54ms/step - loss: 0.3622 - accuracy: 0.7474 - val_loss: 1.0511 - val_accuracy: 0.4858\n",
            "Epoch 257/400\n",
            "80/80 [==============================] - 4s 53ms/step - loss: 0.3590 - accuracy: 0.7474 - val_loss: 1.1043 - val_accuracy: 0.4876\n",
            "Epoch 258/400\n",
            "80/80 [==============================] - 4s 54ms/step - loss: 0.3676 - accuracy: 0.7411 - val_loss: 1.1220 - val_accuracy: 0.4788\n",
            "Epoch 259/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3612 - accuracy: 0.7431 - val_loss: 1.2950 - val_accuracy: 0.4912\n",
            "Epoch 260/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3667 - accuracy: 0.7427 - val_loss: 0.9747 - val_accuracy: 0.4717\n",
            "Epoch 261/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3582 - accuracy: 0.7340 - val_loss: 1.1438 - val_accuracy: 0.4947\n",
            "Epoch 262/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3567 - accuracy: 0.7387 - val_loss: 1.1219 - val_accuracy: 0.4947\n",
            "Epoch 263/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.4166 - accuracy: 0.7344 - val_loss: 0.9461 - val_accuracy: 0.4947\n",
            "Epoch 264/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3771 - accuracy: 0.7423 - val_loss: 1.1356 - val_accuracy: 0.5018\n",
            "Epoch 265/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3622 - accuracy: 0.7423 - val_loss: 1.0623 - val_accuracy: 0.4929\n",
            "Epoch 266/400\n",
            "80/80 [==============================] - 4s 54ms/step - loss: 0.3606 - accuracy: 0.7466 - val_loss: 1.1287 - val_accuracy: 0.5000\n",
            "Epoch 267/400\n",
            "80/80 [==============================] - 4s 54ms/step - loss: 0.3577 - accuracy: 0.7455 - val_loss: 1.1524 - val_accuracy: 0.5018\n",
            "Epoch 268/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3772 - accuracy: 0.7431 - val_loss: 1.1218 - val_accuracy: 0.4770\n",
            "Epoch 269/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3662 - accuracy: 0.7427 - val_loss: 1.0407 - val_accuracy: 0.4982\n",
            "Epoch 270/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3639 - accuracy: 0.7462 - val_loss: 1.1705 - val_accuracy: 0.5035\n",
            "Epoch 271/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3642 - accuracy: 0.7431 - val_loss: 1.0734 - val_accuracy: 0.4947\n",
            "Epoch 272/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3783 - accuracy: 0.7443 - val_loss: 1.0652 - val_accuracy: 0.4770\n",
            "Epoch 273/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3637 - accuracy: 0.7455 - val_loss: 1.1367 - val_accuracy: 0.4841\n",
            "Epoch 274/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3720 - accuracy: 0.7447 - val_loss: 1.0917 - val_accuracy: 0.5071\n",
            "Epoch 275/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3669 - accuracy: 0.7447 - val_loss: 1.0348 - val_accuracy: 0.5000\n",
            "Epoch 276/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3597 - accuracy: 0.7411 - val_loss: 1.0885 - val_accuracy: 0.4841\n",
            "Epoch 277/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3613 - accuracy: 0.7462 - val_loss: 0.9736 - val_accuracy: 0.4823\n",
            "Epoch 278/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3837 - accuracy: 0.7423 - val_loss: 1.1251 - val_accuracy: 0.4841\n",
            "Epoch 279/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3700 - accuracy: 0.7304 - val_loss: 1.1751 - val_accuracy: 0.4876\n",
            "Epoch 280/400\n",
            "80/80 [==============================] - 4s 54ms/step - loss: 0.3623 - accuracy: 0.7451 - val_loss: 1.0431 - val_accuracy: 0.4876\n",
            "Epoch 281/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3723 - accuracy: 0.7498 - val_loss: 1.0483 - val_accuracy: 0.4717\n",
            "Epoch 282/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3813 - accuracy: 0.7411 - val_loss: 1.0107 - val_accuracy: 0.5088\n",
            "Epoch 283/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3606 - accuracy: 0.7312 - val_loss: 1.0348 - val_accuracy: 0.5053\n",
            "Epoch 284/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3643 - accuracy: 0.7332 - val_loss: 0.9178 - val_accuracy: 0.5000\n",
            "Epoch 285/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3583 - accuracy: 0.7439 - val_loss: 1.1193 - val_accuracy: 0.5035\n",
            "Epoch 286/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3703 - accuracy: 0.7458 - val_loss: 1.1587 - val_accuracy: 0.5035\n",
            "Epoch 287/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3841 - accuracy: 0.7447 - val_loss: 0.8753 - val_accuracy: 0.4965\n",
            "Epoch 288/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3734 - accuracy: 0.7360 - val_loss: 0.9811 - val_accuracy: 0.4752\n",
            "Epoch 289/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3582 - accuracy: 0.7375 - val_loss: 1.0596 - val_accuracy: 0.4929\n",
            "Epoch 290/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3746 - accuracy: 0.7411 - val_loss: 0.9604 - val_accuracy: 0.4894\n",
            "Epoch 291/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3623 - accuracy: 0.7411 - val_loss: 1.1127 - val_accuracy: 0.4894\n",
            "Epoch 292/400\n",
            "80/80 [==============================] - 4s 54ms/step - loss: 0.3667 - accuracy: 0.7423 - val_loss: 1.0838 - val_accuracy: 0.5053\n",
            "Epoch 293/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3854 - accuracy: 0.7455 - val_loss: 1.0209 - val_accuracy: 0.5106\n",
            "Epoch 294/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3649 - accuracy: 0.7482 - val_loss: 1.1068 - val_accuracy: 0.4788\n",
            "Epoch 295/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3636 - accuracy: 0.7372 - val_loss: 0.9664 - val_accuracy: 0.5071\n",
            "Epoch 296/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3632 - accuracy: 0.7399 - val_loss: 1.0229 - val_accuracy: 0.4982\n",
            "Epoch 297/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3555 - accuracy: 0.7423 - val_loss: 1.1200 - val_accuracy: 0.4947\n",
            "Epoch 298/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3545 - accuracy: 0.7498 - val_loss: 1.1322 - val_accuracy: 0.4982\n",
            "Epoch 299/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3555 - accuracy: 0.7415 - val_loss: 1.1357 - val_accuracy: 0.5106\n",
            "Epoch 300/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3555 - accuracy: 0.7308 - val_loss: 1.1413 - val_accuracy: 0.5035\n",
            "Epoch 301/400\n",
            "80/80 [==============================] - 5s 56ms/step - loss: 0.3587 - accuracy: 0.7502 - val_loss: 1.0902 - val_accuracy: 0.4912\n",
            "Epoch 302/400\n",
            "80/80 [==============================] - 5s 57ms/step - loss: 0.3660 - accuracy: 0.7462 - val_loss: 1.1049 - val_accuracy: 0.5035\n",
            "Epoch 303/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3588 - accuracy: 0.7415 - val_loss: 1.2859 - val_accuracy: 0.5124\n",
            "Epoch 304/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3649 - accuracy: 0.7470 - val_loss: 0.9521 - val_accuracy: 0.5018\n",
            "Epoch 305/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3640 - accuracy: 0.7336 - val_loss: 0.9682 - val_accuracy: 0.5142\n",
            "Epoch 306/400\n",
            "80/80 [==============================] - 5s 57ms/step - loss: 0.3642 - accuracy: 0.7447 - val_loss: 1.0043 - val_accuracy: 0.4841\n",
            "Epoch 307/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3671 - accuracy: 0.7470 - val_loss: 1.0964 - val_accuracy: 0.4876\n",
            "Epoch 308/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3849 - accuracy: 0.7340 - val_loss: 1.0714 - val_accuracy: 0.4894\n",
            "Epoch 309/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3760 - accuracy: 0.7427 - val_loss: 1.0879 - val_accuracy: 0.4805\n",
            "Epoch 310/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3847 - accuracy: 0.7455 - val_loss: 0.9976 - val_accuracy: 0.5018\n",
            "Epoch 311/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3824 - accuracy: 0.7372 - val_loss: 0.9716 - val_accuracy: 0.4858\n",
            "Epoch 312/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3618 - accuracy: 0.7372 - val_loss: 0.9645 - val_accuracy: 0.5000\n",
            "Epoch 313/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3593 - accuracy: 0.7466 - val_loss: 0.9873 - val_accuracy: 0.5071\n",
            "Epoch 314/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3553 - accuracy: 0.7431 - val_loss: 1.0717 - val_accuracy: 0.5071\n",
            "Epoch 315/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3607 - accuracy: 0.7427 - val_loss: 0.9852 - val_accuracy: 0.4805\n",
            "Epoch 316/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3771 - accuracy: 0.7451 - val_loss: 1.0460 - val_accuracy: 0.4912\n",
            "Epoch 317/400\n",
            "80/80 [==============================] - 5s 56ms/step - loss: 0.3674 - accuracy: 0.7375 - val_loss: 1.0626 - val_accuracy: 0.4912\n",
            "Epoch 318/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3558 - accuracy: 0.7451 - val_loss: 1.1783 - val_accuracy: 0.4929\n",
            "Epoch 319/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3563 - accuracy: 0.7403 - val_loss: 1.0730 - val_accuracy: 0.4894\n",
            "Epoch 320/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3670 - accuracy: 0.7411 - val_loss: 1.1026 - val_accuracy: 0.4735\n",
            "Epoch 321/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3928 - accuracy: 0.7447 - val_loss: 1.0263 - val_accuracy: 0.5053\n",
            "Epoch 322/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3842 - accuracy: 0.7439 - val_loss: 0.9357 - val_accuracy: 0.4788\n",
            "Epoch 323/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.4031 - accuracy: 0.7364 - val_loss: 0.8761 - val_accuracy: 0.4947\n",
            "Epoch 324/400\n",
            "80/80 [==============================] - 5s 59ms/step - loss: 0.3969 - accuracy: 0.7281 - val_loss: 1.0793 - val_accuracy: 0.4929\n",
            "Epoch 325/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3718 - accuracy: 0.7391 - val_loss: 1.0439 - val_accuracy: 0.4965\n",
            "Epoch 326/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3688 - accuracy: 0.7403 - val_loss: 1.1680 - val_accuracy: 0.4929\n",
            "Epoch 327/400\n",
            "80/80 [==============================] - 5s 57ms/step - loss: 0.3625 - accuracy: 0.7482 - val_loss: 1.1734 - val_accuracy: 0.5071\n",
            "Epoch 328/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3755 - accuracy: 0.7466 - val_loss: 1.1010 - val_accuracy: 0.4965\n",
            "Epoch 329/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3705 - accuracy: 0.7403 - val_loss: 0.9837 - val_accuracy: 0.4876\n",
            "Epoch 330/400\n",
            "80/80 [==============================] - 5s 57ms/step - loss: 0.4638 - accuracy: 0.7067 - val_loss: 0.9888 - val_accuracy: 0.4805\n",
            "Epoch 331/400\n",
            "80/80 [==============================] - 5s 57ms/step - loss: 0.3975 - accuracy: 0.7348 - val_loss: 0.9076 - val_accuracy: 0.5035\n",
            "Epoch 332/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3826 - accuracy: 0.7375 - val_loss: 1.1482 - val_accuracy: 0.5000\n",
            "Epoch 333/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3686 - accuracy: 0.7261 - val_loss: 1.1345 - val_accuracy: 0.4929\n",
            "Epoch 334/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3662 - accuracy: 0.7451 - val_loss: 1.1302 - val_accuracy: 0.4947\n",
            "Epoch 335/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3608 - accuracy: 0.7458 - val_loss: 1.1596 - val_accuracy: 0.4982\n",
            "Epoch 336/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3665 - accuracy: 0.7447 - val_loss: 1.1554 - val_accuracy: 0.4858\n",
            "Epoch 337/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3726 - accuracy: 0.7549 - val_loss: 1.1273 - val_accuracy: 0.5018\n",
            "Epoch 338/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3633 - accuracy: 0.7482 - val_loss: 1.0602 - val_accuracy: 0.4982\n",
            "Epoch 339/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3621 - accuracy: 0.7470 - val_loss: 1.1238 - val_accuracy: 0.4965\n",
            "Epoch 340/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3606 - accuracy: 0.7439 - val_loss: 1.0975 - val_accuracy: 0.4929\n",
            "Epoch 341/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3618 - accuracy: 0.7470 - val_loss: 1.0470 - val_accuracy: 0.4982\n",
            "Epoch 342/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3603 - accuracy: 0.7451 - val_loss: 1.0776 - val_accuracy: 0.5018\n",
            "Epoch 343/400\n",
            "80/80 [==============================] - 5s 57ms/step - loss: 0.3581 - accuracy: 0.7411 - val_loss: 1.0741 - val_accuracy: 0.4770\n",
            "Epoch 344/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3601 - accuracy: 0.7387 - val_loss: 1.0679 - val_accuracy: 0.4876\n",
            "Epoch 345/400\n",
            "80/80 [==============================] - 5s 57ms/step - loss: 0.3590 - accuracy: 0.7423 - val_loss: 1.1598 - val_accuracy: 0.4929\n",
            "Epoch 346/400\n",
            "80/80 [==============================] - 5s 57ms/step - loss: 0.3620 - accuracy: 0.7451 - val_loss: 1.1592 - val_accuracy: 0.4735\n",
            "Epoch 347/400\n",
            "80/80 [==============================] - 5s 57ms/step - loss: 0.3603 - accuracy: 0.7486 - val_loss: 1.0490 - val_accuracy: 0.4788\n",
            "Epoch 348/400\n",
            "80/80 [==============================] - 5s 57ms/step - loss: 0.4285 - accuracy: 0.7123 - val_loss: 1.1456 - val_accuracy: 0.4823\n",
            "Epoch 349/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3918 - accuracy: 0.7372 - val_loss: 0.9770 - val_accuracy: 0.4805\n",
            "Epoch 350/400\n",
            "80/80 [==============================] - 5s 58ms/step - loss: 0.3673 - accuracy: 0.7387 - val_loss: 1.0999 - val_accuracy: 0.4947\n",
            "Epoch 351/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3918 - accuracy: 0.7356 - val_loss: 1.0785 - val_accuracy: 0.4841\n",
            "Epoch 352/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3785 - accuracy: 0.7336 - val_loss: 1.0697 - val_accuracy: 0.4841\n",
            "Epoch 353/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3632 - accuracy: 0.7462 - val_loss: 1.1716 - val_accuracy: 0.4805\n",
            "Epoch 354/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3594 - accuracy: 0.7455 - val_loss: 1.1892 - val_accuracy: 0.4752\n",
            "Epoch 355/400\n",
            "80/80 [==============================] - 5s 58ms/step - loss: 0.3584 - accuracy: 0.7451 - val_loss: 1.0850 - val_accuracy: 0.4752\n",
            "Epoch 356/400\n",
            "80/80 [==============================] - 5s 57ms/step - loss: 0.3820 - accuracy: 0.7455 - val_loss: 1.0622 - val_accuracy: 0.4894\n",
            "Epoch 357/400\n",
            "80/80 [==============================] - 5s 58ms/step - loss: 0.3613 - accuracy: 0.7415 - val_loss: 1.1828 - val_accuracy: 0.4823\n",
            "Epoch 358/400\n",
            "80/80 [==============================] - 5s 57ms/step - loss: 0.3617 - accuracy: 0.7451 - val_loss: 1.0401 - val_accuracy: 0.4805\n",
            "Epoch 359/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3710 - accuracy: 0.7455 - val_loss: 0.9432 - val_accuracy: 0.4770\n",
            "Epoch 360/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3676 - accuracy: 0.7435 - val_loss: 1.1591 - val_accuracy: 0.4982\n",
            "Epoch 361/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3628 - accuracy: 0.7451 - val_loss: 1.1170 - val_accuracy: 0.4858\n",
            "Epoch 362/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3592 - accuracy: 0.7431 - val_loss: 1.3365 - val_accuracy: 0.4805\n",
            "Epoch 363/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3637 - accuracy: 0.7462 - val_loss: 1.1185 - val_accuracy: 0.4823\n",
            "Epoch 364/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3602 - accuracy: 0.7439 - val_loss: 1.1741 - val_accuracy: 0.4876\n",
            "Epoch 365/400\n",
            "80/80 [==============================] - 5s 57ms/step - loss: 0.3560 - accuracy: 0.7316 - val_loss: 1.1483 - val_accuracy: 0.5018\n",
            "Epoch 366/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3555 - accuracy: 0.7379 - val_loss: 1.1631 - val_accuracy: 0.4929\n",
            "Epoch 367/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3666 - accuracy: 0.7265 - val_loss: 1.1010 - val_accuracy: 0.4965\n",
            "Epoch 368/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3740 - accuracy: 0.7443 - val_loss: 1.2000 - val_accuracy: 0.4735\n",
            "Epoch 369/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3694 - accuracy: 0.7419 - val_loss: 1.2054 - val_accuracy: 0.4770\n",
            "Epoch 370/400\n",
            "80/80 [==============================] - 5s 57ms/step - loss: 0.3582 - accuracy: 0.7332 - val_loss: 1.1220 - val_accuracy: 0.4912\n",
            "Epoch 371/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3570 - accuracy: 0.7431 - val_loss: 1.1357 - val_accuracy: 0.4929\n",
            "Epoch 372/400\n",
            "80/80 [==============================] - 5s 57ms/step - loss: 0.3573 - accuracy: 0.7506 - val_loss: 1.1765 - val_accuracy: 0.4717\n",
            "Epoch 373/400\n",
            "80/80 [==============================] - 5s 57ms/step - loss: 0.3583 - accuracy: 0.7490 - val_loss: 1.2938 - val_accuracy: 0.4664\n",
            "Epoch 374/400\n",
            "80/80 [==============================] - 5s 58ms/step - loss: 0.3565 - accuracy: 0.7324 - val_loss: 1.2041 - val_accuracy: 0.4841\n",
            "Epoch 375/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.4464 - accuracy: 0.7166 - val_loss: 1.1382 - val_accuracy: 0.4823\n",
            "Epoch 376/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.4075 - accuracy: 0.7423 - val_loss: 1.0246 - val_accuracy: 0.4823\n",
            "Epoch 377/400\n",
            "80/80 [==============================] - 5s 56ms/step - loss: 0.3755 - accuracy: 0.7423 - val_loss: 1.0337 - val_accuracy: 0.4805\n",
            "Epoch 378/400\n",
            "80/80 [==============================] - 5s 56ms/step - loss: 0.3673 - accuracy: 0.7470 - val_loss: 1.1817 - val_accuracy: 0.4858\n",
            "Epoch 379/400\n",
            "80/80 [==============================] - 5s 56ms/step - loss: 0.3675 - accuracy: 0.7423 - val_loss: 1.1452 - val_accuracy: 0.4752\n",
            "Epoch 380/400\n",
            "80/80 [==============================] - 5s 57ms/step - loss: 0.3647 - accuracy: 0.7447 - val_loss: 1.2037 - val_accuracy: 0.4876\n",
            "Epoch 381/400\n",
            "80/80 [==============================] - 5s 57ms/step - loss: 0.3908 - accuracy: 0.7447 - val_loss: 0.9880 - val_accuracy: 0.4894\n",
            "Epoch 382/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3692 - accuracy: 0.7407 - val_loss: 1.0960 - val_accuracy: 0.4752\n",
            "Epoch 383/400\n",
            "80/80 [==============================] - 5s 57ms/step - loss: 0.3648 - accuracy: 0.7391 - val_loss: 1.1433 - val_accuracy: 0.4770\n",
            "Epoch 384/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3589 - accuracy: 0.7419 - val_loss: 1.1953 - val_accuracy: 0.4752\n",
            "Epoch 385/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3710 - accuracy: 0.7506 - val_loss: 1.0876 - val_accuracy: 0.4699\n",
            "Epoch 386/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3682 - accuracy: 0.7466 - val_loss: 1.1607 - val_accuracy: 0.4858\n",
            "Epoch 387/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3693 - accuracy: 0.7415 - val_loss: 1.2126 - val_accuracy: 0.4770\n",
            "Epoch 388/400\n",
            "80/80 [==============================] - 5s 57ms/step - loss: 0.3615 - accuracy: 0.7368 - val_loss: 1.1615 - val_accuracy: 0.4664\n",
            "Epoch 389/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3555 - accuracy: 0.7447 - val_loss: 1.1885 - val_accuracy: 0.4770\n",
            "Epoch 390/400\n",
            "80/80 [==============================] - 5s 58ms/step - loss: 0.3574 - accuracy: 0.7478 - val_loss: 1.2479 - val_accuracy: 0.4752\n",
            "Epoch 391/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3588 - accuracy: 0.7431 - val_loss: 1.1216 - val_accuracy: 0.4752\n",
            "Epoch 392/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3556 - accuracy: 0.7502 - val_loss: 1.0719 - val_accuracy: 0.4752\n",
            "Epoch 393/400\n",
            "80/80 [==============================] - 4s 55ms/step - loss: 0.3718 - accuracy: 0.7458 - val_loss: 1.2198 - val_accuracy: 0.4823\n",
            "Epoch 394/400\n",
            "80/80 [==============================] - 5s 57ms/step - loss: 0.3825 - accuracy: 0.7300 - val_loss: 1.0392 - val_accuracy: 0.4664\n",
            "Epoch 395/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3598 - accuracy: 0.7368 - val_loss: 1.1671 - val_accuracy: 0.4699\n",
            "Epoch 396/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3578 - accuracy: 0.7447 - val_loss: 1.2955 - val_accuracy: 0.4858\n",
            "Epoch 397/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3564 - accuracy: 0.7332 - val_loss: 1.2036 - val_accuracy: 0.4876\n",
            "Epoch 398/400\n",
            "80/80 [==============================] - 5s 57ms/step - loss: 0.3565 - accuracy: 0.7447 - val_loss: 1.2095 - val_accuracy: 0.4823\n",
            "Epoch 399/400\n",
            "80/80 [==============================] - 5s 57ms/step - loss: 0.3613 - accuracy: 0.7423 - val_loss: 1.1484 - val_accuracy: 0.4894\n",
            "Epoch 400/400\n",
            "80/80 [==============================] - 4s 56ms/step - loss: 0.3595 - accuracy: 0.7455 - val_loss: 1.1687 - val_accuracy: 0.4823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvmS3weaPxt4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCMITrArxsTL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06b5ce2e-f7b2-4201-a410-c3f5b88a9e29"
      },
      "source": [
        "Y_test_adv_orig"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, ..., 1, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oY5NceVkwZCc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fb0d9d8-1397-4786-b6b7-25090ff671b3"
      },
      "source": [
        "\n",
        "loss, accuracy = model1.evaluate(X_test1_adv_orig, Y_test1_adv_orig)\n",
        "print('Loss = ' + str(loss))\n",
        "print('Test Accuracy = ' + str(accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36/36 [==============================] - 1s 19ms/step - loss: 0.9752 - accuracy: 0.4717\n",
            "Loss = 0.975220799446106\n",
            "Test Accuracy = 0.4716814160346985\n"
          ]
        }
      ]
    }
  ]
}